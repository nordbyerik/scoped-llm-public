{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_mathematics'], 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'scoper_type': 'activation_steerer', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.1509433962264151), 'out_of_domain_accuracy': np.float64(0.24742268041237114), 'in_domain_control_accuracy': np.float64(0.20754716981132076), 'out_of_domain_control_accuracy': np.float64(0.3711340206185567), 'accuracy': np.float64(0.35333333333333333), 'precision': 0.35333333333333333, 'recall': np.float64(1.0), 'f1_score': np.float64(0.5221674876847291), 'in_domain_accuracy_delta': np.float64(-0.056603773584905676), 'in_domain_accuracy_ttest': np.float64(-1.0000000000000002), 'in_domain_accuracy_p_value': np.float64(0.321941348172834), 'out_of_domain_accuracy_delta': np.float64(-0.12371134020618557), 'out_of_domain_accuracy_ttest': np.float64(-2.5158836081326035), 'out_of_domain_accuracy_p_value': np.float64(0.013532968146384716)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_mathematics'], 'model': 'unsloth/gemma-3-12b-it', 'scoper_type': 'prompt_classification_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.0), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.16981132075471697), 'out_of_domain_control_accuracy': np.float64(0.27835051546391754), 'accuracy': 0.6466666666666666, 'precision': 0, 'recall': 0.0, 'f1_score': 0, 'in_domain_accuracy_delta': np.float64(-0.16981132075471697), 'in_domain_accuracy_ttest': np.float64(-3.2613438390276537), 'in_domain_accuracy_p_value': np.float64(0.001960494157189399), 'out_of_domain_accuracy_delta': np.float64(-0.27835051546391754), 'out_of_domain_accuracy_ttest': np.float64(-6.08511063404532), 'out_of_domain_accuracy_p_value': np.float64(2.3780086377475048e-08)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.029411764705882353), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.8529411764705882), 'out_of_domain_control_accuracy': np.float64(0.5360824742268041), 'accuracy': np.float64(0.7480916030534351), 'precision': 1.0, 'recall': np.float64(0.029411764705882353), 'f1_score': np.float64(0.05714285714285715), 'in_domain_accuracy_delta': np.float64(-0.8235294117647058), 'in_domain_accuracy_ttest': np.float64(-12.409673645990855), 'in_domain_accuracy_p_value': np.float64(5.6108099289785354e-14), 'out_of_domain_accuracy_delta': np.float64(-0.5360824742268041), 'out_of_domain_accuracy_ttest': np.float64(-10.532489417670131), 'out_of_domain_accuracy_p_value': np.float64(1.0726535160630075e-17)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['professional_law', 'jurisprudence', 'business_ethics'], 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'hardened_prompt_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.0), 'out_of_domain_accuracy': np.float64(0.010752688172043012), 'in_domain_control_accuracy': np.float64(0.4189189189189189), 'out_of_domain_control_accuracy': np.float64(0.5698924731182796), 'accuracy': np.float64(0.5449101796407185), 'precision': 0.0, 'recall': np.float64(0.0), 'f1_score': 0, 'in_domain_accuracy_delta': np.float64(-0.4189189189189189), 'in_domain_accuracy_ttest': np.float64(-7.254509423575393), 'in_domain_accuracy_p_value': np.float64(3.504832121705497e-10), 'out_of_domain_accuracy_delta': np.float64(-0.5591397849462366), 'out_of_domain_accuracy_ttest': np.float64(-10.801987170389909), 'out_of_domain_accuracy_p_value': np.float64(4.779434098844938e-18)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'prompt_classification_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.0), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.5578947368421052), 'out_of_domain_control_accuracy': np.float64(0.6161616161616161), 'accuracy': 0.5103092783505154, 'precision': 0, 'recall': 0.0, 'f1_score': 0, 'in_domain_accuracy_delta': np.float64(-0.5578947368421052), 'in_domain_accuracy_ttest': np.float64(-10.89123719414133), 'in_domain_accuracy_p_value': np.float64(2.3830695348625166e-18), 'out_of_domain_accuracy_delta': np.float64(-0.6161616161616161), 'out_of_domain_accuracy_ttest': np.float64(-12.542559127773098), 'out_of_domain_accuracy_p_value': np.float64(4.275412931097101e-22)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.010526315789473684), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.42105263157894735), 'out_of_domain_control_accuracy': np.float64(0.5555555555555556), 'accuracy': np.float64(0.5154639175257731), 'precision': 1.0, 'recall': np.float64(0.010526315789473684), 'f1_score': np.float64(0.020833333333333332), 'in_domain_accuracy_delta': np.float64(-0.4105263157894737), 'in_domain_accuracy_ttest': np.float64(-8.091000291328985), 'in_domain_accuracy_p_value': np.float64(2.056384448699647e-12), 'out_of_domain_accuracy_delta': np.float64(-0.5555555555555556), 'out_of_domain_accuracy_ttest': np.float64(-11.067971810589329), 'out_of_domain_accuracy_p_value': np.float64(5.922472804047736e-19)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'scoper_type': 'hardened_prompt_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.10526315789473684), 'out_of_domain_accuracy': np.float64(0.12121212121212122), 'in_domain_control_accuracy': np.float64(0.35789473684210527), 'out_of_domain_control_accuracy': np.float64(0.3333333333333333), 'accuracy': np.float64(0.4896907216494845), 'precision': 0.4807692307692308, 'recall': np.float64(0.5263157894736842), 'f1_score': np.float64(0.5025125628140704), 'in_domain_accuracy_delta': np.float64(-0.25263157894736843), 'in_domain_accuracy_ttest': np.float64(-5.636900556806369), 'in_domain_accuracy_p_value': np.float64(1.8055553487048422e-07), 'out_of_domain_accuracy_delta': np.float64(-0.2121212121212121), 'out_of_domain_accuracy_ttest': np.float64(-5.136595699937399), 'out_of_domain_accuracy_p_value': np.float64(1.4241717510655974e-06)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_mathematics'], 'model': 'unsloth/gemma-3-12b-it', 'scoper_type': 'hardened_prompt_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.1509433962264151), 'out_of_domain_accuracy': np.float64(0.21649484536082475), 'in_domain_control_accuracy': np.float64(0.1509433962264151), 'out_of_domain_control_accuracy': np.float64(0.21649484536082475), 'accuracy': np.float64(0.35333333333333333), 'precision': 0.35333333333333333, 'recall': np.float64(1.0), 'f1_score': np.float64(0.5221674876847291), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(0.0), 'out_of_domain_accuracy_ttest': np.float64(nan), 'out_of_domain_accuracy_p_value': np.float64(nan)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'unsloth/gemma-3-12b-it', 'scoper_type': 'hardened_prompt_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.29411764705882354), 'out_of_domain_accuracy': np.float64(0.17525773195876287), 'in_domain_control_accuracy': np.float64(0.29411764705882354), 'out_of_domain_control_accuracy': np.float64(0.17525773195876287), 'accuracy': np.float64(0.2595419847328244), 'precision': 0.2595419847328244, 'recall': np.float64(1.0), 'f1_score': np.float64(0.4121212121212121), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(0.0), 'out_of_domain_accuracy_ttest': np.float64(nan), 'out_of_domain_accuracy_p_value': np.float64(nan)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'circuit_breaker_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.0), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.5789473684210527), 'out_of_domain_control_accuracy': np.float64(0.797979797979798), 'accuracy': 0.5103092783505154, 'precision': 0, 'recall': 0.0, 'f1_score': 0, 'in_domain_accuracy_delta': np.float64(-0.5789473684210527), 'in_domain_accuracy_ttest': np.float64(-11.368817000902073), 'in_domain_accuracy_p_value': np.float64(2.363090361561582e-19), 'out_of_domain_accuracy_delta': np.float64(-0.797979797979798), 'out_of_domain_accuracy_ttest': np.float64(-19.674857051577277), 'out_of_domain_accuracy_p_value': np.float64(8.269432147521786e-36)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.0), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.6470588235294118), 'out_of_domain_control_accuracy': np.float64(0.5670103092783505), 'accuracy': 0.7404580152671756, 'precision': 0, 'recall': 0.0, 'f1_score': 0, 'in_domain_accuracy_delta': np.float64(-0.6470588235294118), 'in_domain_accuracy_ttest': np.float64(-7.7781745930520225), 'in_domain_accuracy_p_value': np.float64(5.8139729397838095e-09), 'out_of_domain_accuracy_delta': np.float64(-0.5670103092783505), 'out_of_domain_accuracy_ttest': np.float64(-11.212238211627762), 'out_of_domain_accuracy_p_value': np.float64(3.806086778860597e-19)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'scoper_type': 'activation_steerer', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.2647058823529412), 'out_of_domain_accuracy': np.float64(0.15463917525773196), 'in_domain_control_accuracy': np.float64(0.4117647058823529), 'out_of_domain_control_accuracy': np.float64(0.31958762886597936), 'accuracy': np.float64(0.2595419847328244), 'precision': 0.2595419847328244, 'recall': np.float64(1.0), 'f1_score': np.float64(0.4121212121212121), 'in_domain_accuracy_delta': np.float64(-0.14705882352941174), 'in_domain_accuracy_ttest': np.float64(-1.7134593839651469), 'in_domain_accuracy_p_value': np.float64(0.09600901640129171), 'out_of_domain_accuracy_delta': np.float64(-0.1649484536082474), 'out_of_domain_accuracy_ttest': np.float64(-3.4439808160040406), 'out_of_domain_accuracy_p_value': np.float64(0.0008512551376527682)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_mathematics'], 'model': 'unsloth/gemma-3-12b-it', 'scoper_type': 'activation_steerer', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.22641509433962265), 'out_of_domain_accuracy': np.float64(0.23711340206185566), 'in_domain_control_accuracy': np.float64(0.22641509433962265), 'out_of_domain_control_accuracy': np.float64(0.23711340206185566), 'accuracy': np.float64(0.35333333333333333), 'precision': 0.35333333333333333, 'recall': np.float64(1.0), 'f1_score': np.float64(0.5221674876847291), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(0.0), 'out_of_domain_accuracy_ttest': np.float64(nan), 'out_of_domain_accuracy_p_value': np.float64(nan)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_mathematics'], 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'scoper_type': 'activation_steerer', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.22641509433962265), 'out_of_domain_accuracy': np.float64(0.25773195876288657), 'in_domain_control_accuracy': np.float64(0.3018867924528302), 'out_of_domain_control_accuracy': np.float64(0.36082474226804123), 'accuracy': np.float64(0.35333333333333333), 'precision': 0.35333333333333333, 'recall': np.float64(1.0), 'f1_score': np.float64(0.5221674876847291), 'in_domain_accuracy_delta': np.float64(-0.07547169811320753), 'in_domain_accuracy_ttest': np.float64(-1.428011094590833), 'in_domain_accuracy_p_value': np.float64(0.1592673674950467), 'out_of_domain_accuracy_delta': np.float64(-0.10309278350515466), 'out_of_domain_accuracy_ttest': np.float64(-2.2841609628806427), 'out_of_domain_accuracy_p_value': np.float64(0.02456525343344035)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.0), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.8529411764705882), 'out_of_domain_control_accuracy': np.float64(0.5463917525773195), 'accuracy': 0.7404580152671756, 'precision': 0, 'recall': 0.0, 'f1_score': 0, 'in_domain_accuracy_delta': np.float64(-0.8529411764705882), 'in_domain_accuracy_ttest': np.float64(-13.834738884417009), 'in_domain_accuracy_p_value': np.float64(2.7253458667985425e-15), 'out_of_domain_accuracy_delta': np.float64(-0.5463917525773195), 'out_of_domain_accuracy_ttest': np.float64(-10.753434969179086), 'out_of_domain_accuracy_p_value': np.float64(3.61410504547519e-18)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'unsloth/gemma-3-12b-it', 'scoper_type': 'activation_steerer', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.23529411764705882), 'out_of_domain_accuracy': np.float64(0.21649484536082475), 'in_domain_control_accuracy': np.float64(0.23529411764705882), 'out_of_domain_control_accuracy': np.float64(0.21649484536082475), 'accuracy': np.float64(0.2595419847328244), 'precision': 0.2595419847328244, 'recall': np.float64(1.0), 'f1_score': np.float64(0.4121212121212121), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(0.0), 'out_of_domain_accuracy_ttest': np.float64(nan), 'out_of_domain_accuracy_p_value': np.float64(nan)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_mathematics'], 'model': 'unsloth/gemma-3-12b-it', 'scoper_type': 'hardened_prompt_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.20754716981132076), 'out_of_domain_accuracy': np.float64(0.25773195876288657), 'in_domain_control_accuracy': np.float64(0.20754716981132076), 'out_of_domain_control_accuracy': np.float64(0.25773195876288657), 'accuracy': np.float64(0.35333333333333333), 'precision': 0.35333333333333333, 'recall': np.float64(1.0), 'f1_score': np.float64(0.5221674876847291), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(0.0), 'out_of_domain_accuracy_ttest': np.float64(nan), 'out_of_domain_accuracy_p_value': np.float64(nan)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'scoper_type': 'prompt_classification_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.010526315789473684), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.37894736842105264), 'out_of_domain_control_accuracy': np.float64(0.47474747474747475), 'accuracy': np.float64(0.520618556701031), 'precision': 1.0, 'recall': np.float64(0.021052631578947368), 'f1_score': np.float64(0.041237113402061855), 'in_domain_accuracy_delta': np.float64(-0.368421052631579), 'in_domain_accuracy_ttest': np.float64(-7.091110124286593), 'in_domain_accuracy_p_value': np.float64(2.447963238098558e-10), 'out_of_domain_accuracy_delta': np.float64(-0.47474747474747475), 'out_of_domain_accuracy_ttest': np.float64(-9.411531388510749), 'out_of_domain_accuracy_p_value': np.float64(2.2926914678785084e-15)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.0), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.7647058823529411), 'out_of_domain_control_accuracy': np.float64(0.6082474226804123), 'accuracy': np.float64(0.7480916030534351), 'precision': 1.0, 'recall': np.float64(0.029411764705882353), 'f1_score': np.float64(0.05714285714285715), 'in_domain_accuracy_delta': np.float64(-0.7647058823529411), 'in_domain_accuracy_ttest': np.float64(-10.356157588603986), 'in_domain_accuracy_p_value': np.float64(6.690930996656612e-12), 'out_of_domain_accuracy_delta': np.float64(-0.6082474226804123), 'out_of_domain_accuracy_ttest': np.float64(-12.208711298861456), 'out_of_domain_accuracy_p_value': np.float64(3.015733211306798e-21)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['professional_law', 'jurisprudence', 'business_ethics'], 'model': 'deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'scoper_type': 'activation_steerer', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.24324324324324326), 'out_of_domain_accuracy': np.float64(0.24731182795698925), 'in_domain_control_accuracy': np.float64(0.24324324324324326), 'out_of_domain_control_accuracy': np.float64(0.3333333333333333), 'accuracy': np.float64(0.4431137724550898), 'precision': 0.4431137724550898, 'recall': np.float64(1.0), 'f1_score': np.float64(0.6141078838174274), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(0.0), 'in_domain_accuracy_p_value': np.float64(1.0), 'out_of_domain_accuracy_delta': np.float64(-0.08602150537634407), 'out_of_domain_accuracy_ttest': np.float64(-1.7235824986541854), 'out_of_domain_accuracy_p_value': np.float64(0.08814239036018218)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_mathematics'], 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'circuit_breaker_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.0), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.32075471698113206), 'out_of_domain_control_accuracy': np.float64(0.5876288659793815), 'accuracy': 0.6466666666666666, 'precision': 0, 'recall': 0.0, 'f1_score': 0, 'in_domain_accuracy_delta': np.float64(-0.32075471698113206), 'in_domain_accuracy_ttest': np.float64(-4.955356249106168), 'in_domain_accuracy_p_value': np.float64(8.04994447201256e-06), 'out_of_domain_accuracy_delta': np.float64(-0.5876288659793815), 'out_of_domain_accuracy_ttest': np.float64(-11.696153213770756), 'out_of_domain_accuracy_p_value': np.float64(3.596673157543575e-20)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'hardened_prompt_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.0), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.6176470588235294), 'out_of_domain_control_accuracy': np.float64(0.5876288659793815), 'accuracy': np.float64(0.7251908396946565), 'precision': 0.0, 'recall': np.float64(0.0), 'f1_score': 0, 'in_domain_accuracy_delta': np.float64(-0.6176470588235294), 'in_domain_accuracy_ttest': np.float64(-7.3012117013337114), 'in_domain_accuracy_p_value': np.float64(2.2248485303012472e-08), 'out_of_domain_accuracy_delta': np.float64(-0.5876288659793815), 'out_of_domain_accuracy_ttest': np.float64(-11.696153213770756), 'out_of_domain_accuracy_p_value': np.float64(3.596673157543575e-20)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['professional_law', 'jurisprudence', 'business_ethics'], 'model': 'unsloth/gemma-3-12b-it', 'scoper_type': 'hardened_prompt_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.33783783783783783), 'out_of_domain_accuracy': np.float64(0.25806451612903225), 'in_domain_control_accuracy': np.float64(0.33783783783783783), 'out_of_domain_control_accuracy': np.float64(0.25806451612903225), 'accuracy': np.float64(0.4431137724550898), 'precision': 0.4431137724550898, 'recall': np.float64(1.0), 'f1_score': np.float64(0.6141078838174274), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(0.0), 'out_of_domain_accuracy_ttest': np.float64(nan), 'out_of_domain_accuracy_p_value': np.float64(nan)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_mathematics'], 'model': 'unsloth/gemma-3-12b-it', 'scoper_type': 'hardened_prompt_scoper', 'test_examples': 100, 'training_examples': 1000}, 'metrics': {'in_domain_accuracy': np.float64(0.22641509433962265), 'out_of_domain_accuracy': np.float64(0.26804123711340205), 'in_domain_control_accuracy': np.float64(0.22641509433962265), 'out_of_domain_control_accuracy': np.float64(0.26804123711340205), 'accuracy': np.float64(0.35333333333333333), 'precision': 0.35333333333333333, 'recall': np.float64(1.0), 'f1_score': np.float64(0.5221674876847291), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(0.0), 'out_of_domain_accuracy_ttest': np.float64(nan), 'out_of_domain_accuracy_p_value': np.float64(nan)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['professional_law', 'jurisprudence', 'business_ethics'], 'model': 'unsloth/Llama-3.2-1B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.4), 'out_of_domain_accuracy': np.float64(0.1), 'in_domain_control_accuracy': np.float64(0.4), 'out_of_domain_control_accuracy': np.float64(0.7), 'accuracy': np.float64(0.9), 'precision': 0.8333333333333334, 'recall': np.float64(1.0), 'f1_score': np.float64(0.9090909090909091), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(-0.6), 'out_of_domain_accuracy_ttest': np.float64(-3.674234614174767), 'out_of_domain_accuracy_p_value': np.float64(0.005121072764272635)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.6), 'out_of_domain_accuracy': np.float64(0.5), 'in_domain_control_accuracy': np.float64(0.7), 'out_of_domain_control_accuracy': np.float64(1.0), 'accuracy': np.float64(0.7), 'precision': 0.6428571428571429, 'recall': np.float64(0.9), 'f1_score': np.float64(0.75), 'in_domain_accuracy_delta': np.float64(-0.09999999999999998), 'in_domain_accuracy_ttest': np.float64(-1.0), 'in_domain_accuracy_p_value': np.float64(0.34343639613791355), 'out_of_domain_accuracy_delta': np.float64(-0.5), 'out_of_domain_accuracy_ttest': np.float64(-2.9999999999999996), 'out_of_domain_accuracy_p_value': np.float64(0.014956363910414203)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'unsloth/Meta-Llama-3.1-8B', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.4), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.6), 'out_of_domain_control_accuracy': np.float64(0.5), 'accuracy': np.float64(0.85), 'precision': 1.0, 'recall': np.float64(0.7), 'f1_score': np.float64(0.8235294117647058), 'in_domain_accuracy_delta': np.float64(-0.19999999999999996), 'in_domain_accuracy_ttest': np.float64(-1.4999999999999998), 'in_domain_accuracy_p_value': np.float64(0.16785065605707492), 'out_of_domain_accuracy_delta': np.float64(-0.5), 'out_of_domain_accuracy_ttest': np.float64(-2.9999999999999996), 'out_of_domain_accuracy_p_value': np.float64(0.014956363910414203)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.7), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.6), 'out_of_domain_control_accuracy': np.float64(0.1), 'accuracy': np.float64(1.0), 'precision': 1.0, 'recall': np.float64(1.0), 'f1_score': np.float64(1.0), 'in_domain_accuracy_delta': np.float64(0.09999999999999998), 'in_domain_accuracy_ttest': np.float64(1.0), 'in_domain_accuracy_p_value': np.float64(0.34343639613791355), 'out_of_domain_accuracy_delta': np.float64(-0.1), 'out_of_domain_accuracy_ttest': np.float64(-1.0), 'out_of_domain_accuracy_p_value': np.float64(0.34343639613791355)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_chemistry'], 'model': 'unsloth/Meta-Llama-3.1-8B', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.6), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.7), 'out_of_domain_control_accuracy': np.float64(0.8), 'accuracy': np.float64(0.95), 'precision': 1.0, 'recall': np.float64(0.9), 'f1_score': np.float64(0.9473684210526316), 'in_domain_accuracy_delta': np.float64(-0.09999999999999998), 'in_domain_accuracy_ttest': np.float64(-1.0), 'in_domain_accuracy_p_value': np.float64(0.34343639613791355), 'out_of_domain_accuracy_delta': np.float64(-0.8), 'out_of_domain_accuracy_ttest': np.float64(-5.999999999999999), 'out_of_domain_accuracy_p_value': np.float64(0.0002024993220676408)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'unsloth/Llama-3.2-1B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.1), 'out_of_domain_accuracy': np.float64(0.2), 'in_domain_control_accuracy': np.float64(0.3), 'out_of_domain_control_accuracy': np.float64(0.6), 'accuracy': np.float64(0.65), 'precision': 0.6666666666666666, 'recall': np.float64(0.6), 'f1_score': np.float64(0.631578947368421), 'in_domain_accuracy_delta': np.float64(-0.19999999999999998), 'in_domain_accuracy_ttest': np.float64(-1.4999999999999998), 'in_domain_accuracy_p_value': np.float64(0.16785065605707492), 'out_of_domain_accuracy_delta': np.float64(-0.39999999999999997), 'out_of_domain_accuracy_ttest': np.float64(-2.4494897427831783), 'out_of_domain_accuracy_p_value': np.float64(0.03678749787978613)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_chemistry'], 'model': 'unsloth/Llama-3.2-1B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.6), 'out_of_domain_accuracy': np.float64(0.1), 'in_domain_control_accuracy': np.float64(0.6), 'out_of_domain_control_accuracy': np.float64(0.6), 'accuracy': np.float64(0.9), 'precision': 0.8333333333333334, 'recall': np.float64(1.0), 'f1_score': np.float64(0.9090909090909091), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(-0.5), 'out_of_domain_accuracy_ttest': np.float64(-2.23606797749979), 'out_of_domain_accuracy_p_value': np.float64(0.052177242798819065)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['professional_law', 'jurisprudence', 'business_ethics'], 'model': 'unsloth/Meta-Llama-3.1-8B', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.6), 'out_of_domain_accuracy': np.float64(0.1), 'in_domain_control_accuracy': np.float64(0.6), 'out_of_domain_control_accuracy': np.float64(0.5), 'accuracy': np.float64(0.85), 'precision': 0.7692307692307693, 'recall': np.float64(1.0), 'f1_score': np.float64(0.8695652173913044), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(-0.4), 'out_of_domain_accuracy_ttest': np.float64(-2.449489742783178), 'out_of_domain_accuracy_p_value': np.float64(0.03678749787978613)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'unsloth/Llama-3.2-1B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.2), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.3), 'out_of_domain_control_accuracy': np.float64(0.3), 'accuracy': np.float64(0.7), 'precision': 0.7, 'recall': np.float64(0.7), 'f1_score': np.float64(0.7), 'in_domain_accuracy_delta': np.float64(-0.09999999999999998), 'in_domain_accuracy_ttest': np.float64(-1.0), 'in_domain_accuracy_p_value': np.float64(0.34343639613791355), 'out_of_domain_accuracy_delta': np.float64(-0.3), 'out_of_domain_accuracy_ttest': np.float64(-1.9639610121239315), 'out_of_domain_accuracy_p_value': np.float64(0.0811261888458405)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.4), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.5), 'out_of_domain_control_accuracy': np.float64(0.4), 'accuracy': np.float64(0.8), 'precision': 1.0, 'recall': np.float64(0.6), 'f1_score': np.float64(0.7499999999999999), 'in_domain_accuracy_delta': np.float64(-0.09999999999999998), 'in_domain_accuracy_ttest': np.float64(-1.0), 'in_domain_accuracy_p_value': np.float64(0.34343639613791355), 'out_of_domain_accuracy_delta': np.float64(-0.4), 'out_of_domain_accuracy_ttest': np.float64(-2.4494897427831783), 'out_of_domain_accuracy_p_value': np.float64(0.03678749787978613)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.3), 'out_of_domain_accuracy': np.float64(0.1), 'in_domain_control_accuracy': np.float64(0.6), 'out_of_domain_control_accuracy': np.float64(0.7), 'accuracy': np.float64(0.75), 'precision': 0.8571428571428571, 'recall': np.float64(0.6), 'f1_score': np.float64(0.7058823529411764), 'in_domain_accuracy_delta': np.float64(-0.3), 'in_domain_accuracy_ttest': np.float64(-1.963961012123931), 'in_domain_accuracy_p_value': np.float64(0.08112618884584075), 'out_of_domain_accuracy_delta': np.float64(-0.6), 'out_of_domain_accuracy_ttest': np.float64(-3.674234614174767), 'out_of_domain_accuracy_p_value': np.float64(0.005121072764272635)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.3), 'out_of_domain_accuracy': np.float64(0.2), 'in_domain_control_accuracy': np.float64(0.4), 'out_of_domain_control_accuracy': np.float64(0.8), 'accuracy': np.float64(0.8), 'precision': 0.75, 'recall': np.float64(0.9), 'f1_score': np.float64(0.8181818181818182), 'in_domain_accuracy_delta': np.float64(-0.10000000000000003), 'in_domain_accuracy_ttest': np.float64(-1.0), 'in_domain_accuracy_p_value': np.float64(0.34343639613791355), 'out_of_domain_accuracy_delta': np.float64(-0.6000000000000001), 'out_of_domain_accuracy_ttest': np.float64(-3.6742346141747673), 'out_of_domain_accuracy_p_value': np.float64(0.005121072764272635)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'unsloth/Meta-Llama-3.1-8B', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.6), 'out_of_domain_accuracy': np.float64(0.1), 'in_domain_control_accuracy': np.float64(0.7), 'out_of_domain_control_accuracy': np.float64(1.0), 'accuracy': np.float64(0.8), 'precision': 0.875, 'recall': np.float64(0.7), 'f1_score': np.float64(0.7777777777777777), 'in_domain_accuracy_delta': np.float64(-0.09999999999999998), 'in_domain_accuracy_ttest': np.float64(-1.0), 'in_domain_accuracy_p_value': np.float64(0.34343639613791355), 'out_of_domain_accuracy_delta': np.float64(-0.9), 'out_of_domain_accuracy_ttest': np.float64(-9.0), 'out_of_domain_accuracy_p_value': np.float64(8.538051223166285e-06)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['professional_law', 'jurisprudence', 'business_ethics'], 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.4), 'out_of_domain_accuracy': np.float64(0.1), 'in_domain_control_accuracy': np.float64(0.4), 'out_of_domain_control_accuracy': np.float64(0.5), 'accuracy': np.float64(0.9), 'precision': 0.9, 'recall': np.float64(0.9), 'f1_score': np.float64(0.9), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(-0.4), 'out_of_domain_accuracy_ttest': np.float64(-2.4494897427831783), 'out_of_domain_accuracy_p_value': np.float64(0.03678749787978613)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'unsloth/Llama-3.2-3B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.8), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.8), 'out_of_domain_control_accuracy': np.float64(0.6), 'accuracy': np.float64(1.0), 'precision': 1.0, 'recall': np.float64(1.0), 'f1_score': np.float64(1.0), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(-0.6), 'out_of_domain_accuracy_ttest': np.float64(-3.674234614174767), 'out_of_domain_accuracy_p_value': np.float64(0.005121072764272635)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_chemistry'], 'model': 'unsloth/Llama-3.2-1B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.4), 'out_of_domain_accuracy': np.float64(0.1), 'in_domain_control_accuracy': np.float64(0.6), 'out_of_domain_control_accuracy': np.float64(0.5), 'accuracy': np.float64(0.8), 'precision': 0.875, 'recall': np.float64(0.7), 'f1_score': np.float64(0.7777777777777777), 'in_domain_accuracy_delta': np.float64(-0.19999999999999996), 'in_domain_accuracy_ttest': np.float64(-1.4999999999999998), 'in_domain_accuracy_p_value': np.float64(0.16785065605707492), 'out_of_domain_accuracy_delta': np.float64(-0.4), 'out_of_domain_accuracy_ttest': np.float64(-2.449489742783178), 'out_of_domain_accuracy_p_value': np.float64(0.03678749787978613)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_chemistry'], 'model': 'unsloth/Meta-Llama-3.1-8B', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.3), 'out_of_domain_accuracy': np.float64(0.0), 'in_domain_control_accuracy': np.float64(0.5), 'out_of_domain_control_accuracy': np.float64(0.7), 'accuracy': np.float64(0.85), 'precision': 0.8888888888888888, 'recall': np.float64(0.8), 'f1_score': np.float64(0.8421052631578948), 'in_domain_accuracy_delta': np.float64(-0.2), 'in_domain_accuracy_ttest': np.float64(-1.4999999999999998), 'in_domain_accuracy_p_value': np.float64(0.16785065605707492), 'out_of_domain_accuracy_delta': np.float64(-0.7), 'out_of_domain_accuracy_ttest': np.float64(-4.58257569495584), 'out_of_domain_accuracy_p_value': np.float64(0.0013229505842674947)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'unsloth/Llama-3.2-1B-Instruct', 'scoper_type': 'linear_probe_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.5), 'out_of_domain_accuracy': np.float64(0.2), 'in_domain_control_accuracy': np.float64(0.7), 'out_of_domain_control_accuracy': np.float64(0.7), 'accuracy': np.float64(0.9), 'precision': 0.8333333333333334, 'recall': np.float64(1.0), 'f1_score': np.float64(0.9090909090909091), 'in_domain_accuracy_delta': np.float64(-0.19999999999999996), 'in_domain_accuracy_ttest': np.float64(-1.4999999999999998), 'in_domain_accuracy_p_value': np.float64(0.16785065605707492), 'out_of_domain_accuracy_delta': np.float64(-0.49999999999999994), 'out_of_domain_accuracy_ttest': np.float64(-2.9999999999999996), 'out_of_domain_accuracy_p_value': np.float64(0.014956363910414203)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': 'stem', 'model': 'Qwen/Qwen3-32B', 'scoper_type': 'hardened_prompt_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.5), 'out_of_domain_accuracy': np.float64(0.2), 'in_domain_control_accuracy': np.float64(0.2), 'out_of_domain_control_accuracy': np.float64(0.3), 'accuracy': np.float64(0.5), 'precision': 0.5, 'recall': np.float64(1.0), 'f1_score': np.float64(0.6666666666666666), 'in_domain_accuracy_delta': np.float64(0.3), 'in_domain_accuracy_ttest': np.float64(1.963961012123931), 'in_domain_accuracy_p_value': np.float64(0.08112618884584075), 'out_of_domain_accuracy_delta': np.float64(-0.09999999999999998), 'out_of_domain_accuracy_ttest': np.float64(-0.5570860145311556), 'out_of_domain_accuracy_p_value': np.float64(0.5910512317836047)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['high_school_chemistry'], 'model': 'Qwen/Qwen3-32B', 'scoper_type': 'activation_steerer', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.1), 'out_of_domain_accuracy': np.float64(0.5), 'in_domain_control_accuracy': np.float64(0.1), 'out_of_domain_control_accuracy': np.float64(0.5), 'accuracy': np.float64(0.5), 'precision': 0.5, 'recall': np.float64(1.0), 'f1_score': np.float64(0.6666666666666666), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(0.0), 'out_of_domain_accuracy_ttest': np.float64(nan), 'out_of_domain_accuracy_p_value': np.float64(nan)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'Qwen/Qwen3-32B', 'scoper_type': 'activation_steerer', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.4), 'out_of_domain_accuracy': np.float64(0.1), 'in_domain_control_accuracy': np.float64(0.4), 'out_of_domain_control_accuracy': np.float64(0.1), 'accuracy': np.float64(0.5), 'precision': 0.5, 'recall': np.float64(1.0), 'f1_score': np.float64(0.6666666666666666), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(0.0), 'out_of_domain_accuracy_ttest': np.float64(nan), 'out_of_domain_accuracy_p_value': np.float64(nan)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['professional_law', 'jurisprudence', 'business_ethics'], 'model': 'Qwen/Qwen3-32B', 'scoper_type': 'hardened_prompt_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.2), 'out_of_domain_accuracy': np.float64(0.4), 'in_domain_control_accuracy': np.float64(0.2), 'out_of_domain_control_accuracy': np.float64(0.4), 'accuracy': np.float64(0.5), 'precision': 0.5, 'recall': np.float64(1.0), 'f1_score': np.float64(0.6666666666666666), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(0.0), 'in_domain_accuracy_p_value': np.float64(1.0), 'out_of_domain_accuracy_delta': np.float64(0.0), 'out_of_domain_accuracy_ttest': np.float64(0.0), 'out_of_domain_accuracy_p_value': np.float64(1.0)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['world_religions'], 'model': 'unsloth/Llama-3.2-1B', 'scoper_type': 'circuit_breaker_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.7), 'out_of_domain_accuracy': np.float64(0.3), 'in_domain_control_accuracy': np.float64(0.7), 'out_of_domain_control_accuracy': np.float64(0.3), 'accuracy': np.float64(0.5), 'precision': 0.5, 'recall': np.float64(1.0), 'f1_score': np.float64(0.6666666666666666), 'in_domain_accuracy_delta': np.float64(0.0), 'in_domain_accuracy_ttest': np.float64(nan), 'in_domain_accuracy_p_value': np.float64(nan), 'out_of_domain_accuracy_delta': np.float64(0.0), 'out_of_domain_accuracy_ttest': np.float64(nan), 'out_of_domain_accuracy_p_value': np.float64(nan)}}
{'config': {'batch_size': 2, 'dataset': 'mmlu', 'domains': ['professional_law', 'jurisprudence', 'business_ethics'], 'model': 'google/gemma-2-27b-it', 'scoper_type': 'hardened_prompt_scoper', 'test_percentage': 0.2, 'training_examples': 100}, 'metrics': {'in_domain_accuracy': np.float64(0.5), 'out_of_domain_accuracy': np.float64(0.7), 'in_domain_control_accuracy': np.float64(0.4), 'out_of_domain_control_accuracy': np.float64(0.8), 'accuracy': np.float64(0.5), 'precision': 0.5, 'recall': np.float64(1.0), 'f1_score': np.float64(0.6666666666666666), 'in_domain_accuracy_delta': np.float64(0.09999999999999998), 'in_domain_accuracy_ttest': np.float64(1.0), 'in_domain_accuracy_p_value': np.float64(0.34343639613791355), 'out_of_domain_accuracy_delta': np.float64(-0.10000000000000009), 'out_of_domain_accuracy_ttest': np.float64(-0.5570860145311556), 'out_of_domain_accuracy_p_value': np.float64(0.5910512317836047)}}
