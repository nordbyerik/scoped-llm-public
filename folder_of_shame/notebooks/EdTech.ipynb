{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pip Installs for Quantization\n",
        "In order to squeeze better performance out of our models, we want to make use of some quanitzation techniques. I'm just now learning about these so I'll be trying a few out. Source: https://generativeai.pub/practical-guide-of-llm-quantization-gptq-awq-bitsandbytes-and-unsloth-bdeaa2c0bbf6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Found existing installation: transformers 4.50.3\n",
            "Uninstalling transformers-4.50.3:\n",
            "  Successfully uninstalled transformers-4.50.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "Found existing installation: huggingface-hub 0.30.1\n",
            "Uninstalling huggingface-hub-0.30.1:\n",
            "  Successfully uninstalled huggingface-hub-0.30.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install auto-gptq trl bitsandbytes>=0.39.0 datasets -q\n",
        "!pip install accelerate transformers optimum onnx onnxruntime -q\n",
        "!pip uninstall transformers -y\n",
        "!pip install transformers==4.46.1 -q\n",
        "\n",
        "!pip uninstall huggingface_hub -y\n",
        "!pip install huggingface_hub==0.25 -q\n",
        "\n",
        "!pip install scikit-learn matplotlib -q\n",
        "\n",
        "!pip install flash-attn --no-build-isolation -q\n",
        "!pip install -U transformers unsloth bitsandbytes flash-attn -q\n",
        "!pip install git+https://github.com/unslothai/unsloth.git -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Google and HuggingFace Drive Boilerplate\n",
        "This has boilerplate code which connects our collab to google drive. This allows us to persist some of the data that we create & to store large datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Not in colab, just loading hugging face\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  from google.colab import drive\n",
        "  # drive.mount('/content/drive')\n",
        "except:\n",
        "  print(\"Not in colab, just loading hugging face\")\n",
        "\n",
        "\n",
        "from huggingface_hub import login, HfApi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Upload To Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_path = '/content/drive/My Drive/persuade_corpus_2.0_train.csv'\n",
        "repo_id = \"ErikNordby/EdTech\"  # Replace with your username and desired dataset name\n",
        "\n",
        "UPLOAD = False\n",
        "if UPLOAD:\n",
        "    api = HfApi(token=os.environ.get(\"HUGGINGFACE_TOKEN\"))\n",
        "    api.upload_file(\n",
        "        path_or_fileobj=data_path,\n",
        "        path_in_repo=\"persuade_corpus_2.0_train.csv\",  # Name of the file in the repository\n",
        "        repo_id=repo_id,\n",
        "        repo_type=\"dataset\"\n",
        "    )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download from Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a829d7fdb02d43d98d8aac73c18d8ac5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "persuade_corpus_2.0_train.csv:   0%|          | 0.00/617M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_3373/1655391147.py:13: DtypeWarning: Columns (17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  essays = pd.read_csv(file_path)\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import pandas as pd\n",
        "\n",
        "# Specify the repository information\n",
        "repo_id = \"ErikNordby/EdTech\"  # Replace with the actual repository\n",
        "filename = \"persuade_corpus_2.0_train.csv\"  # Replace with the actual filename in the repository\n",
        "\n",
        "# Download the file\n",
        "login(token=os.environ.get(\"HUGGINGFACE_TOKEN\"))\n",
        "file_path = hf_hub_download(repo_id=repo_id, filename=filename, repo_type=\"dataset\")\n",
        "\n",
        "# Load the CSV into a pandas DataFrame\n",
        "essays = pd.read_csv(file_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Dataset with Persuade Dataset\n",
        "\n",
        "The persuade dataset contains essays written by students from grades 8-12. This section loads up those essays & creates 2 different versions of prompts. First we create prompts with just the plain data & a request for feedback. Secondly, we create a dataset which asks for feedback which follows the criteria put forth for good feedback by Macfarlane-Dick (Cite Here)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clean Data\n",
        "Here we're just cleaning up the data since it's a pretty large dataset. For right now, we're just going to try getting away with 10 essays which are relatively short"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "short_essays = essays[essays['essay_word_count'] < 500]\n",
        "short_essays = short_essays.sample(n=5)\n",
        "short_essays = short_essays.reset_index()\n",
        "short_essays_text = short_essays['full_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Prompts\n",
        "Here we're creating the prompts that we're going to be using. Currently, I'm just using some pretty simple example which break out the desired behavior into different types. The thought is that maybe we can isolate these behaviors into different vectors which can be combined or compared to a more general \"good advice\" prompt set. Note: This is for my course that I'm taking, leaving this in but not using the essay prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_feedback_prompts(dataset, feedback_prompts):\n",
        "  essay_prompts = []\n",
        "  for feedback_prompt in feedback_prompts:\n",
        "    prompts = short_essays_text.apply(lambda x: \"Essay: \" + x.strip() + ' Request: ' + feedback_prompt)\n",
        "    essay_prompts.append(prompts)\n",
        "  return pd.concat(essay_prompts).values\n",
        "\n",
        "negative_examples = [\n",
        "    \"Critique this harshly\",\n",
        "    \"Give mean feedback\",\n",
        "    \"Critique this\",\n",
        "    \"Tear this apart with your feedback\",\n",
        "    \"Be brutally honest about the flaws\",\n",
        "    \"Don't hold back any criticism\",\n",
        "    \"Give your most ruthless feedback\",\n",
        "    \"Point out everything wrong with this\",\n",
        "    \"What's the harshest criticism you can give?\",\n",
        "    \"Provide only negative feedback\",\n",
        "    \"Destroy this with your critique\",\n",
        "    \"Tell me why this is terrible\",\n",
        "    \"What makes this completely unacceptable?\",\n",
        "    \"Give feedback as if you hate this\",\n",
        "    \"List all the failures in this work\",\n",
        "    \"Be merciless in your assessment\",\n",
        "    \"What would a harsh critic say?\",\n",
        "    \"Criticize this as severely as possible\",\n",
        "    \"Tell me every single flaw you can find\",\n",
        "    \"What's the worst thing about this?\"\n",
        "]\n",
        "\n",
        "encouraging_examples = [\n",
        "    \"Give good feedback for this which is supportive\",\n",
        "    \"Give good feedback which is encouraging\",\n",
        "    \"Give feedback which highlights areas of success\",\n",
        "    \"Provide constructive feedback with a positive tone\",\n",
        "    \"Share supportive comments that build confidence\",\n",
        "    \"Offer feedback that recognizes strengths while suggesting improvements\",\n",
        "    \"Give encouraging feedback that inspires continued effort\",\n",
        "    \"Highlight what works well in this\",\n",
        "    \"Share positive reinforcement with gentle suggestions\",\n",
        "    \"Provide uplifting feedback that acknowledges the effort\",\n",
        "    \"What's particularly impressive about this?\",\n",
        "    \"Tell me what you love about this work\",\n",
        "    \"Give feedback that celebrates the achievements here\",\n",
        "    \"Share what makes this stand out in a positive way\",\n",
        "    \"Offer encouragement that builds on existing strengths\",\n",
        "    \"What would make someone feel proud about this work?\",\n",
        "    \"Give feedback as a supportive mentor would\",\n",
        "    \"Share what's most promising about this approach\",\n",
        "    \"How would you compliment this work to boost confidence?\",\n",
        "    \"Provide feedback that appreciates the creativity shown\"\n",
        "]\n",
        "\n",
        "\n",
        "# Bad examples\n",
        "negative_essay_prompts = apply_feedback_prompts(short_essays_text, negative_examples).tolist()\n",
        "\n",
        "# Good examples\n",
        "encourageing_essay_prompts = apply_feedback_prompts(short_essays_text, encouraging_examples).tolist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create hooks for the activation\n",
        "We want to be able to both have visibility into the activations and to also be able to modify the activations. So, we're creating hooks that can be attached to layers of the model which allow us to do that."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "class ActivationHook:\n",
        "    \"\"\"Hook to extract activations from a specific layer of a transformer model.\"\"\"\n",
        "    def __init__(self, module, layer_name, transformation_function=None):\n",
        "        self.activations = None\n",
        "        self.layer_name = layer_name\n",
        "        self.transformation_function = transformation_function if transformation_function is not None else lambda x: x\n",
        "\n",
        "        # Register forward hoo_k\n",
        "        if \"mlp\" in self.layer_name:\n",
        "            module.register_forward_hook(self.hook_mlp)\n",
        "        elif \"self_attn\" in self.layer_name:\n",
        "            module.register_forward_hook(self.hook_attention)\n",
        "        elif \"residual\" in self.layer_name:\n",
        "            module.register_forward_hook(self.hook_residual)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported layer: {layer_name}\")\n",
        "\n",
        "    def set_transformation_function(self, transformation_function):\n",
        "        self.transformation_function = transformation_function\n",
        "\n",
        "    def hook_mlp(self, module, input, output):\n",
        "        self.activations = output.detach()\n",
        "        return self.transformation_function(output)\n",
        "\n",
        "    def hook_attention(self, module, input, output):\n",
        "        self.activations = output[0].detach()\n",
        "        output_updated = self.transformation_function(output[0])\n",
        "        output = (output_updated,) + output[1:]\n",
        "        return output\n",
        "\n",
        "    def hook_residual(self, module, input, output):\n",
        "        self.activations = output[0].detach()\n",
        "        output_updated = self.transformation_function(output[0])\n",
        "        output = (output_updated,) + output[1:]\n",
        "        return output\n",
        "\n",
        "    def print_activations(self):\n",
        "        print(self.activations)\n",
        "\n",
        "    def clear_activations(self):\n",
        "        self.activations = None\n",
        "\n",
        "    def clear_transformation_function(self):\n",
        "        self.transformation_function = lambda x: x\n",
        "\n",
        "    def clear(self):\n",
        "        self.clear_activations()\n",
        "        self.clear_transformation_function()\n",
        "\n",
        "    def __del__(self):\n",
        "        self.clear()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setting Up Steering Class\n",
        "Now that we have our arrays of string and our code to hook the activation, we now need to load in the language model and write some code to actually handle the steering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Setting Up Steering Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F # Import functional for softmax\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GPTQConfig\n",
        "from abc import ABC, abstractmethod\n",
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from accelerate import infer_auto_device_map, init_empty_weights, load_checkpoint_and_dispatch\n",
        "from optimum.onnxruntime import ORTModelForCausalLM\n",
        "from optimum.bettertransformer import BetterTransformer\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "class LLMSteerer:\n",
        "    def __init__(self, model_name):\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_quant_type=\"nf4\",  # Higher quality quantization\n",
        "            bnb_4bit_use_double_quant=True  # Further reduces memory usage\n",
        "        )\n",
        "\n",
        "        # Replace your model loading in LLMSteerer.__init__ with:\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            quantization_config=quantization_config,\n",
        "            device_map=\"auto\",\n",
        "            attn_implementation=\"eager\",  # Avoid Flash Attention error\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "        # OR Option B: Use BetterTransformer for memory-efficient attention\n",
        "        # self.model = BetterTransformer.transform(base_model)\n",
        "\n",
        "    def generate(self, prompt, max_length=100):\n",
        "        # Process input with optimized inference\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        # Use generate method of optimized model\n",
        "        with torch.no_grad(), torch.cuda.amp.autocast():  # Use mixed precision\n",
        "            outputs = self.model.generate(\n",
        "                input_ids=inputs.input_ids,\n",
        "                attention_mask=inputs.attention_mask,\n",
        "                max_new_tokens=max_length,\n",
        "                pad_token_id=self.tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode and return generated text\n",
        "        generated_text = self.tokenizer.decode(\n",
        "            outputs[0][inputs.input_ids.shape[1]:],\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        # Clean up to free memory\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return generated_text\n",
        "\n",
        "\n",
        "class PromptSteerer(LLMSteerer):\n",
        "    def __init__(self, model, prompt_template, hidden_size=128):\n",
        "        super().__init__(model, hidden_size)\n",
        "        self.prompt_template = prompt_template\n",
        "\n",
        "    def generate(self, prompt):\n",
        "        full_prompt = self.prompt_template.format(prompt)\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        return self.model.generate(\n",
        "            input_ids=inputs.input_ids.to(model.device),\n",
        "            attention_mask=inputs.attention_mask.to(model.device) if 'attention_mask' in inputs else None,\n",
        "            max_new_tokens=5,  # Generate a few tokens to catch the answer\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "\n",
        "class ActivationController(LLMSteerer):\n",
        "    def __init__(self, model, selected_blocks=None, hidden_size=128):\n",
        "        super().__init__(model)\n",
        "\n",
        "        self.CACHE_DIR = \"/content/my_caches\"\n",
        "        self.hooks = {}\n",
        "        num_layers = self.model.config.num_hidden_layers\n",
        "\n",
        "        # Dynamically create layer names for every 5th layer\n",
        "        self.selected_layers = []\n",
        "        for i in range(0, num_layers, 5):  # Step by 5\n",
        "            # Add the last layer if we're not exactly divisible by 5\n",
        "            if i >= num_layers:\n",
        "                break\n",
        "            self.selected_layers.append(f'model.layers.{i}.self_attn')\n",
        "            self.selected_layers.append(f'model.layers.{i}.mlp')\n",
        "\n",
        "        # Add the last layer if it wasn't included\n",
        "        if (num_layers - 1) % 5 != 0:\n",
        "            self.selected_layers.append(f'model.layers.{num_layers-1}.self_attn')\n",
        "            self.selected_layers.append(f'model.layers.{num_layers-1}.mlp')\n",
        "\n",
        "        if selected_blocks is not None:\n",
        "            self.selected_layers = selected_blocks\n",
        "\n",
        "        def get_layer(layer_name, layer_module):\n",
        "            layer_idx = int(layer_name.split('.')[-2])\n",
        "            if \"GPTNeoXForCausalLM\" in self.model.config.architectures:\n",
        "                layer = self.model.gpt_neox.layers[layer_idx]\n",
        "            else:\n",
        "                layer = self.model.model.layers[layer_idx]\n",
        "\n",
        "            if 'mlp' in layer_name:\n",
        "                return layer.mlp\n",
        "            elif 'self_attn' in layer_name:\n",
        "                if \"GPTNeoXForCausalLM\" in self.model.config.architectures:\n",
        "                    return layer.attention\n",
        "                else:\n",
        "                    return layer.self_attn\n",
        "            elif 'residual' in layer_name:\n",
        "                return layer\n",
        "\n",
        "        # Add hooks\n",
        "        for layer_name in self.selected_layers:\n",
        "            if 'mlp' in layer_name:\n",
        "                module = get_layer(layer_name, \"mlp\")\n",
        "            elif 'self_attn' in layer_name:\n",
        "                module = get_layer(layer_name, \"self_attn\")\n",
        "            elif 'residual' in layer_name:\n",
        "                module = get_layer(layer_name, \"residual\")\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported layer: {layer_name}\")\n",
        "\n",
        "            self.hooks[layer_name] = ActivationHook(module, layer_name)\n",
        "\n",
        "    def extract_activation(self, layer_name, hook, aggregation_calc):\n",
        "        \"\"\"Extracts and aggregates activation, minimizing CPU RAM usage.\"\"\"\n",
        "        if hook.activations is None:\n",
        "            return None # Or handle appropriately\n",
        "\n",
        "        activations_gpu = hook.activations # Keep on original device (likely GPU)\n",
        "\n",
        "        # Perform aggregation on the GPU tensor\n",
        "        if aggregation_calc == \"mean\":\n",
        "            # Mean across sequence length (dim=1 assuming batch, seq, hidden)\n",
        "            # Keepdims=True might be useful if subsequent code expects shape (batch, 1, hidden)\n",
        "            aggregated_gpu = torch.mean(activations_gpu, dim=1)\n",
        "        elif aggregation_calc == \"max\":\n",
        "            # Max across sequence length\n",
        "            aggregated_gpu = torch.max(activations_gpu, dim=1).values # .values is important for max\n",
        "        elif aggregation_calc == \"last\":\n",
        "            # Take the last token's activation along the sequence dim (dim=1)\n",
        "            aggregated_gpu = activations_gpu[:, -1, :]\n",
        "        else:\n",
        "            # Default to last token or raise error\n",
        "            print(f\"Warning: Unsupported aggregation '{aggregation_calc}'. Defaulting to 'last'.\")\n",
        "            aggregated_gpu = activations_gpu[:, :, :]\n",
        "\n",
        "        # Now move only the smaller, aggregated tensor to CPU and convert to numpy\n",
        "        # Use detach() just in case gradients were somehow attached\n",
        "        aggregated_cpu_numpy = aggregated_gpu.detach().cpu().numpy()\n",
        "\n",
        "        return aggregated_cpu_numpy\n",
        "\n",
        "    def extract_activations(self, texts, device='cuda', aggregation_calc=\"all\"):\n",
        "        \"\"\"Extract activations from specified layers for a list of input texts.\"\"\"\n",
        "        # Extract activations for each text\n",
        "        all_activations = {layer_name: [] for layer_name in self.selected_layers}\n",
        "\n",
        "        for text_index in range(len(texts)):\n",
        "            text = texts[text_index]\n",
        "\n",
        "            # Tokenize and process through model\n",
        "            inputs = self.tokenizer(text, return_tensors='pt')\n",
        "            inputs = inputs.to(self.device)\n",
        "            with torch.no_grad():\n",
        "                self.model(**inputs)\n",
        "\n",
        "            # Collect activations from hooks\n",
        "            for layer_name, hook in self.hooks.items():\n",
        "                if hook.activations is not None:\n",
        "                    last_token_activation = self.extract_activation(layer_name, hook, aggregation_calc)\n",
        "                    all_activations[layer_name].append(last_token_activation)\n",
        "                    hook.clear()\n",
        "\n",
        "\n",
        "        return all_activations\n",
        "\n",
        "\n",
        "    def set_transformation_function(self, layer_name, transformation_function):\n",
        "        self.hooks[layer_name].set_transformation_function(transformation_function)\n",
        "\n",
        "    def get_transformation_function(self, layer_name):\n",
        "        return self.hooks[layer_name].transformation_function\n",
        "\n",
        "    def clear_hooks(self):\n",
        "        for hook in self.hooks.values():\n",
        "            hook.clear()\n",
        "\n",
        "    def generate(self, prompt, max_length = 1):\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "        generated = self.model.generate(\n",
        "            input_ids=inputs.input_ids.to(self.model.device),\n",
        "            attention_mask=inputs.attention_mask.to(self.model.device) if 'attention_mask' in inputs else None,\n",
        "            max_new_tokens=max_length,  # Generate a few tokens to catch the answer\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "\n",
        "        input_length = inputs.input_ids.shape[1]\n",
        "        generated_tokens = generated[0][input_length:]\n",
        "        generated_text = self.tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "        return generated_text\n",
        "\n",
        "    def __del__(self):\n",
        "        self.clear_hooks()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Averaging Method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ActAddSteerer(ActivationController):\n",
        "    def __init__(self, model, selected_blocks=None, hidden_size=128):\n",
        "        super().__init__(model)\n",
        "\n",
        "    # Inside the ActAddSteerer class\n",
        "    def extract_average_activations(self, texts, device='cuda', aggregation_calc=\"last\"):\n",
        "        \"\"\"Calculates average activations directly without storing all intermediates.\"\"\"\n",
        "        if not self.selected_layers:\n",
        "            return {}\n",
        "\n",
        "        # Use the modified extract_activation from Solution 1 above\n",
        "        # Initialize sums and counts\n",
        "        activation_sums = {layer_name: None for layer_name in self.selected_layers}\n",
        "        activation_counts = {layer_name: 0 for layer_name in self.selected_layers}\n",
        "        example_shape = {layer_name: None for layer_name in self.selected_layers} # To store shape info\n",
        "\n",
        "        # Use tqdm for progress tracking if desired\n",
        "        from tqdm.notebook import tqdm\n",
        "\n",
        "        for text in tqdm(texts, desc=\"Extracting Avg Activations\"):\n",
        "            inputs = self.tokenizer(text, return_tensors='pt', truncation=True, max_length=self.model.config.max_position_embeddings // 2).to(device) # Add truncation\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    _ = self.model(**inputs) # Run forward pass, hooks capture activations\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Error processing text: {text[:50]}... Error: {e}\")\n",
        "                continue # Skip this text if model forward pass fails\n",
        "\n",
        "            for layer_name, hook in self.hooks.items():\n",
        "                # Use the MEMORY-EFFICIENT extract_activation from Solution 1\n",
        "                aggregated_numpy = self.extract_activation(layer_name, hook, aggregation_calc)\n",
        "\n",
        "                if aggregated_numpy is not None:\n",
        "                    current_act_flat = aggregated_numpy.flatten() # Ensure 1D for summing\n",
        "\n",
        "                    if activation_sums[layer_name] is None:\n",
        "                        activation_sums[layer_name] = current_act_flat\n",
        "                        example_shape[layer_name] = aggregated_numpy.shape # Store original shape\n",
        "                    else:\n",
        "                        # Ensure consistent shapes before adding\n",
        "                        if activation_sums[layer_name].shape == current_act_flat.shape:\n",
        "                            activation_sums[layer_name] += current_act_flat\n",
        "                        else:\n",
        "                            print(f\"Warning: Shape mismatch for layer {layer_name}. Skipping update.\")\n",
        "                            continue # Skip if shapes don't match (shouldn't happen with consistent aggregation)\n",
        "\n",
        "                    activation_counts[layer_name] += 1\n",
        "                hook.clear() # Clear hook activation storage\n",
        "\n",
        "        # Calculate averages\n",
        "        average_activations = {}\n",
        "        for layer_name in self.selected_layers:\n",
        "            if activation_counts[layer_name] > 0:\n",
        "                avg = activation_sums[layer_name] / activation_counts[layer_name]\n",
        "                # Reshape back to original aggregated shape if needed (e.g., (1, hidden_dim))\n",
        "                original_shape = example_shape[layer_name]\n",
        "                if original_shape:\n",
        "                    try:\n",
        "                        average_activations[layer_name] = avg.reshape(original_shape)\n",
        "                    except ValueError:\n",
        "                        print(f\"Warning: Could not reshape avg for layer {layer_name} back to {original_shape}. Keeping flat.\")\n",
        "                        average_activations[layer_name] = avg # Keep flat if reshape fails\n",
        "                else:\n",
        "                    average_activations[layer_name] = avg # Keep flat if shape wasn't stored\n",
        "            else:\n",
        "                print(f\"Warning: No activations collected for layer {layer_name}. Returning None.\")\n",
        "                average_activations[layer_name] = None # Or np.zeros(...) if preferred\n",
        "\n",
        "        # Clean up large intermediate sums immediately\n",
        "        del activation_sums\n",
        "        del activation_counts\n",
        "        del example_shape\n",
        "        torch.cuda.empty_cache()\n",
        "        import gc\n",
        "        gc.collect()\n",
        "\n",
        "        return average_activations\n",
        "\n",
        "    def extract_average_diff(self, positive_texts, negative_texts, device='cuda'):\n",
        "        positive_activations = self.extract_average_activations(positive_texts, device)\n",
        "        negative_activations = self.extract_average_activations(negative_texts, device)\n",
        "\n",
        "        average_diff_activations = {}\n",
        "        for layer_name, activations in positive_activations.items():\n",
        "            average_diff_activations[layer_name] = positive_activations[layer_name] - negative_activations[layer_name]\n",
        "\n",
        "        return average_diff_activations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.manifold import TSNE\n",
        "# from umap import UMAP # Uncomment if you have umap-learn installed and prefer UMAP\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "from functools import partial # Useful for setting transformation functions\n",
        "\n",
        "# Assuming your existing classes (ActivationHook, LLMSteerer, ActivationController, etc.)\n",
        "# are defined above or imported.\n",
        "\n",
        "# --- Visualization Functions ---\n",
        "\n",
        "def plot_activation_projection(all_activations, layer_name, labels, method='tsne', perplexity=30, n_neighbors=15, min_dist=0.1, title_suffix=\"\"):\n",
        "    \"\"\"\n",
        "    Plots a 2D projection (t-SNE or UMAP) of activations for a specific layer,\n",
        "    colored by labels (e.g., positive/negative).\n",
        "\n",
        "    Args:\n",
        "        all_activations (dict): Dictionary from layer_name to list of activations (numpy arrays).\n",
        "                                Assumes each activation in the list is for one text input,\n",
        "                                likely shape (hidden_dim,) or (1, hidden_dim).\n",
        "        layer_name (str): The layer whose activations to plot.\n",
        "        labels (list): A list of labels (e.g., 'positive', 'negative') corresponding\n",
        "                       to the order of activations in all_activations[layer_name].\n",
        "        method (str): 'tsne' or 'umap'.\n",
        "        perplexity (float): Perplexity for t-SNE.\n",
        "        n_neighbors (int): n_neighbors for UMAP.\n",
        "        min_dist (float): min_dist for UMAP.\n",
        "        title_suffix (str): Optional suffix for the plot title.\n",
        "    \"\"\"\n",
        "    if layer_name not in all_activations or not all_activations[layer_name]:\n",
        "        print(f\"No activations found for layer {layer_name}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    activations_np = np.array(all_activations[layer_name]).squeeze()\n",
        "\n",
        "\n",
        "    print(f\"Projecting activations for layer: {layer_name} (Shape: {activations_np.shape}) using {method.upper()}...\")\n",
        "\n",
        "    if method == 'tsne':\n",
        "        reducer = TSNE(n_components=2, random_state=42, perplexity=min(perplexity, activations_np.shape[0] - 1)) # Perplexity must be less than n_samples\n",
        "    elif method == 'umap':\n",
        "        try:\n",
        "             from umap import UMAP # Check import locally\n",
        "             reducer = UMAP(n_components=2, random_state=42, n_neighbors=min(n_neighbors, activations_np.shape[0] - 1), min_dist=min_dist)\n",
        "        except ImportError:\n",
        "             print(\"UMAP not installed. Install umap-learn. Falling back to t-SNE.\")\n",
        "             reducer = TSNE(n_components=2, random_state=42, perplexity=min(perplexity, activations_np.shape[0] - 1))\n",
        "    else:\n",
        "        raise ValueError(\"Method must be 'tsne' or 'umap'\")\n",
        "\n",
        "    projections = reducer.fit_transform(activations_np)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.scatterplot(x=projections[:, 0], y=projections[:, 1], hue=labels, palette='viridis', s=50, alpha=0.8)\n",
        "    plt.title(f'{method.upper()} Projection of Activations\\nLayer: {layer_name}{title_suffix}')\n",
        "    plt.xlabel(f'{method.upper()} Dimension 1')\n",
        "    plt.ylabel(f'{method.upper()} Dimension 2')\n",
        "    plt.legend(title='Input Type')\n",
        "    plt.grid(True, linestyle='--', alpha=0.6)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_steering_vector(steering_vector, layer_name, k=20):\n",
        "    \"\"\"\n",
        "    Plots the steering vector values and a bar chart of the top K dimensions by magnitude.\n",
        "\n",
        "    Args:\n",
        "        steering_vector (np.ndarray): The calculated steering vector (1D).\n",
        "        layer_name (str): The layer name for the title.\n",
        "        k (int): Number of top dimensions to show in the bar chart.\n",
        "    \"\"\"\n",
        "    if steering_vector is None or steering_vector.size == 0:\n",
        "        print(f\"Invalid steering vector for layer {layer_name}\")\n",
        "        return\n",
        "\n",
        "    steering_vector = steering_vector.flatten() # Ensure 1D\n",
        "\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    fig.suptitle(f\"Steering Vector Analysis - Layer: {layer_name}\", fontsize=14)\n",
        "\n",
        "    # Plot 1: Line plot of the vector values\n",
        "    axs[0].plot(steering_vector)\n",
        "    axs[0].set_title('Steering Vector Values')\n",
        "    axs[0].set_xlabel('Dimension Index')\n",
        "    axs[0].set_ylabel('Value')\n",
        "    axs[0].grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # Plot 2: Top K dimensions by absolute value\n",
        "    if k > 0 and steering_vector.size > 0:\n",
        "         k = min(k, steering_vector.size) # Adjust k if vector is smaller\n",
        "         top_k_indices = np.argsort(np.abs(steering_vector))[-k:]\n",
        "         top_k_values = steering_vector[top_k_indices]\n",
        "         dim_labels = [f'Dim {i}' for i in top_k_indices]\n",
        "\n",
        "         colors = ['red' if v < 0 else 'blue' for v in top_k_values]\n",
        "         axs[1].bar(range(k), top_k_values, color=colors)\n",
        "         axs[1].set_xticks(range(k))\n",
        "         axs[1].set_xticklabels(dim_labels, rotation=90)\n",
        "         axs[1].set_title(f'Top {k} Dimensions by Magnitude')\n",
        "         axs[1].set_ylabel('Value')\n",
        "         axs[1].grid(True, axis='y', linestyle='--', alpha=0.6)\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout to prevent title overlap\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_steering_vector_norms(average_diff_activations):\n",
        "    \"\"\"\n",
        "    Plots the L2 norm of steering vectors across different layers.\n",
        "\n",
        "    Args:\n",
        "        average_diff_activations (dict): Dictionary from layer_name to steering vector (np.ndarray).\n",
        "    \"\"\"\n",
        "    if not average_diff_activations:\n",
        "        print(\"No steering vectors provided.\")\n",
        "        return\n",
        "\n",
        "    layer_names = list(average_diff_activations.keys())\n",
        "    norms = [np.linalg.norm(vec) for vec in average_diff_activations.values() if vec is not None]\n",
        "    valid_layer_names = [name for name, vec in average_diff_activations.items() if vec is not None]\n",
        "\n",
        "    if not norms:\n",
        "        print(\"No valid steering vectors found to calculate norms.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar(range(len(norms)), norms, tick_label=valid_layer_names)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.ylabel('L2 Norm (Magnitude)')\n",
        "    plt.title('Steering Vector Norms Across Layers')\n",
        "    plt.grid(True, axis='y', linestyle='--', alpha=0.6)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_steering_vector_similarity(average_diff_activations):\n",
        "    \"\"\"\n",
        "    Plots a heatmap of the cosine similarity between steering vectors from different layers.\n",
        "\n",
        "    Args:\n",
        "        average_diff_activations (dict): Dictionary from layer_name to steering vector (np.ndarray).\n",
        "    \"\"\"\n",
        "    if not average_diff_activations:\n",
        "        print(\"No steering vectors provided.\")\n",
        "        return\n",
        "\n",
        "    layer_names = list(average_diff_activations.keys())\n",
        "    vectors = [vec.flatten() for vec in average_diff_activations.values() if vec is not None]\n",
        "    valid_layer_names = [name for name, vec in average_diff_activations.items() if vec is not None]\n",
        "\n",
        "    if len(vectors) < 2:\n",
        "        print(\"Need at least two valid steering vectors to compare similarity.\")\n",
        "        return\n",
        "\n",
        "    # Ensure all vectors are 1D for cosine similarity calculation if needed,\n",
        "    # although cosine_similarity handles 2D arrays row-wise. Stacking is safer.\n",
        "    try:\n",
        "         vector_matrix = np.stack(vectors) # Shape (n_layers, hidden_dim)\n",
        "    except ValueError as e:\n",
        "         print(f\"Error stacking vectors, likely due to inconsistent dimensions: {e}\")\n",
        "         # Attempt to pad or resize if necessary, or just report error\n",
        "         max_len = max(v.shape[0] for v in vectors)\n",
        "         padded_vectors = []\n",
        "         for v in vectors:\n",
        "             if v.shape[0] < max_len:\n",
        "                 pad_width = max_len - v.shape[0]\n",
        "                 padded_v = np.pad(v, (0, pad_width), 'constant')\n",
        "                 padded_vectors.append(padded_v)\n",
        "                 print(f\"Warning: Padded vector for layer {valid_layer_names[len(padded_vectors)-1]}\")\n",
        "             else:\n",
        "                 padded_vectors.append(v)\n",
        "         vector_matrix = np.stack(padded_vectors)\n",
        "         # return # Alternatively, decide whether to proceed with potentially padded vectors\n",
        "\n",
        "\n",
        "    similarity_matrix = cosine_similarity(vector_matrix)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(similarity_matrix, annot=True, cmap='coolwarm', fmt=\".2f\",\n",
        "                xticklabels=valid_layer_names, yticklabels=valid_layer_names)\n",
        "    plt.xticks(rotation=90)\n",
        "    plt.yticks(rotation=0)\n",
        "    plt.title('Cosine Similarity Between Steering Vectors Across Layers')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def compare_generation(steerer, prompts, steering_params, max_length=50):\n",
        "    \"\"\"\n",
        "    Generates text with and without steering for comparison.\n",
        "\n",
        "    Args:\n",
        "        steerer (ActivationController): An initialized ActivationController instance.\n",
        "        prompts (list): A list of input prompts.\n",
        "        steering_params (dict): A dict mapping layer_name to {'vector': np.ndarray, 'coeff': float}.\n",
        "                                The steering vector should be a numpy array.\n",
        "        max_length (int): Max new tokens to generate.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: DataFrame with prompts, unsteered, and steered outputs.\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    device = steerer.device\n",
        "\n",
        "    # Prepare steering vectors on the correct device\n",
        "    steering_vectors_gpu = {}\n",
        "    for layer_name, params in steering_params.items():\n",
        "        vector_np = params['vector']\n",
        "        # Ensure vector is compatible shape for broadcasting (e.g., (1, 1, hidden_dim))\n",
        "        # This depends on the activation shape inside the hook. Let's assume the hook\n",
        "        # expects a transformation on an activation of shape (batch, seq, hidden).\n",
        "        # If vector_np is (hidden_dim,), reshape it.\n",
        "        if vector_np.ndim == 1:\n",
        "             # Reshape for broadcasting: (1, 1, hidden_dim) might be needed\n",
        "             # Or let the hook handle it if it's simple addition.\n",
        "             # For safety, let's add batch and sequence dimensions.\n",
        "             vector_reshaped = vector_np.reshape(1, 1, -1)\n",
        "        else:\n",
        "             # Assume it's already correctly shaped if not 1D\n",
        "             vector_reshaped = vector_np\n",
        "\n",
        "        steering_vectors_gpu[layer_name] = torch.tensor(vector_reshaped, dtype=torch.float16 if 'cuda' in device else torch.float32).to(device) # Use float16 on GPU\n",
        "\n",
        "\n",
        "    for prompt in prompts:\n",
        "        print(f\"\\nProcessing prompt: '{prompt}'\")\n",
        "\n",
        "        # --- Unsteered Generation ---\n",
        "        print(\"  Generating (Unsteered)...\")\n",
        "        steerer.clear_hooks() # Ensure no leftover transformations\n",
        "        text_unsteered = steerer.generate(prompt, max_length=max_length)\n",
        "        print(f\"    Unsteered: {text_unsteered}\")\n",
        "\n",
        "        # --- Steered Generation ---\n",
        "        print(\"  Generating (Steered)...\")\n",
        "        steerer.clear_hooks() # Clear just in case, then set new ones\n",
        "        for layer_name, params in steering_params.items():\n",
        "            if layer_name in steerer.hooks:\n",
        "                coeff = params['coeff']\n",
        "                vector_gpu = steering_vectors_gpu[layer_name]\n",
        "\n",
        "                # Define the transformation function using the pre-moved tensor\n",
        "                # This lambda will be called by the hook with the activation tensor\n",
        "                def transformation_func(activation_tensor, vec=vector_gpu, c=coeff):\n",
        "                    # Add the steering vector (broadcasting should handle dimensions)\n",
        "                    # Ensure dtype compatibility if using AMP\n",
        "                    return activation_tensor.to(vec.dtype) + c * vec\n",
        "\n",
        "                steerer.set_transformation_function(layer_name, transformation_func)\n",
        "                print(f\"    Applied steering to {layer_name} with coeff {coeff}\")\n",
        "            else:\n",
        "                print(f\"    Warning: Layer {layer_name} not found in hooks.\")\n",
        "\n",
        "        text_steered = steerer.generate(prompt, max_length=max_length)\n",
        "        print(f\"    Steered:   {text_steered}\")\n",
        "\n",
        "        results.append({\n",
        "            \"Prompt\": prompt,\n",
        "            \"Unsteered Output\": text_unsteered,\n",
        "            \"Steered Output\": text_steered\n",
        "        })\n",
        "\n",
        "        # Clear hooks for the next prompt\n",
        "        steerer.clear_hooks()\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# --- Example Usage ---\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # This is placeholder execution logic. Replace with your actual model loading\n",
        "    # and data preparation. Assume you have:\n",
        "    # - `model_name`: Name of the HF model\n",
        "    # - `positive_texts`, `negative_texts`: Lists of strings for extracting vectors\n",
        "    # - `test_prompts`: List of strings to test generation steering\n",
        "\n",
        "    print(\"Setting up steerers (Example - replace with your model)...\")\n",
        "    MODEL_NAME = \"unsloth/Llama-3.3-70B-Instruct\" # Example - use a model you have access to\n",
        "\n",
        "    selected_blocks = ['model.layers.30.self_attn', 'model.layers.30.mlp']\n",
        "    avg_steerer = ActAddSteerer(MODEL_NAME, selected_blocks=selected_blocks) # Uses LLMSteerer internally\n",
        "\n",
        "\n",
        "\n",
        "    # --- 1. Extract Activations (Example Data) ---\n",
        "    indices = np.random.choice(len(negative_essay_prompts), size=100, replace=False)\n",
        "    positive_texts = [encourageing_essay_prompts[i] for i in indices] # Using list comprehension for proper indexing\n",
        "    negative_texts = [negative_essay_prompts[i] for i in indices] # Same fix for negative_texts\n",
        "    all_texts = positive_texts + negative_texts\n",
        "    labels = ['positive'] * len(positive_texts) + ['negative'] * len(negative_texts)\n",
        "\n",
        "    all_activations = avg_steerer.extract_activations(all_texts, aggregation_calc=\"other\")\n",
        "\n",
        "    target_layer = avg_steerer.layer_names[len(avg_steerer.layer_names)-1] # Example: middle layer ml\n",
        "\n",
        "    plot_activation_projection(all_activations, target_layer, labels, method='tsne')\n",
        "\n",
        "    all_activations = None # Clear to free up memory\n",
        "\n",
        "    # --- 3. Calculate and Visualize Steering Vector ---\n",
        "    # Use default aggregation (last token) for steering vector calculation\n",
        "    average_diff_activations = avg_steerer.extract_average_diff(positive_texts, negative_texts)\n",
        "\n",
        "    plot_steering_vector(average_diff_activations[target_layer], target_layer, k=20)\n",
        "\n",
        "    print(\"\\nPlotting steering vector norms...\")\n",
        "    plot_steering_vector_norms(average_diff_activations)\n",
        "\n",
        "    print(\"\\nPlotting steering vector similarity...\")\n",
        "    plot_steering_vector_similarity(average_diff_activations)\n",
        "\n",
        "\n",
        "    # --- 4. Compare Generation ---\n",
        "    print(\"\\nComparing steered vs. unsteered generation...\")\n",
        "    test_prompts = [\n",
        "        \"The weather today is\",\n",
        "        \"I thought the presentation was\",\n",
        "        \"This policy change seems\",\n",
        "        \"User Input: I really like strawberries.\\nAI Response: Strawberries are\", # Example for steering tone\n",
        "        \"User Input: I really hate spiders.\\nAI Response: Spiders are\"\n",
        "    ]\n",
        "\n",
        "    # Define steering parameters (apply the positive vector with a coefficient)\n",
        "    # Apply to one or more layers where the vector norm was significant\n",
        "    steering_params = {}\n",
        "    steering_coeff = 10 # Example coefficient - TUNE THIS!\n",
        "    if target_layer in average_diff_activations and average_diff_activations[target_layer] is not None:\n",
        "        steering_params[target_layer] = {\n",
        "            'vector': average_diff_activations[target_layer], # The pos-neg vector\n",
        "            'coeff': steering_coeff\n",
        "        }\n",
        "\n",
        "    comparison_df = compare_generation(avg_steerer, test_prompts, steering_params, max_length=20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load up model and get example\n",
        "model = \"unsloth/Llama-3.2-3B\"\n",
        "activation_steerer = ActAddSteerer(model)\n",
        "output1 = activation_steerer.generate(negative_essay_prompts[5], max_length=250)\n",
        "print(output1)\n",
        "\n",
        "# Load up the diffs\n",
        "indices = np.random.choice(len(negative_essay_prompts), size=10, replace=False)\n",
        "rand_good = encourageing_essay_prompts[indices]\n",
        "rand_negative = negative_essay_prompts[indices]\n",
        "\n",
        "average_diff = activation_steerer.extract_average_diff(rand_good, rand_negative)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for layer_name in activation_steerer.layer_names:\n",
        "    print(layer_name)\n",
        "    added_tensor = torch.tensor(average_diff[layer_name]).to(activation_steerer.model.device)\n",
        "    def add_to_last_index(x, added_tensor=added_tensor):\n",
        "        x[:, -1, :] += added_tensor\n",
        "        return x\n",
        "\n",
        "    activation_steerer.set_transformation_function(layer_name, add_to_last_index)\n",
        "\n",
        "print(activation_steerer.generate(negative_essay_prompts[5], max_length=250))\n",
        "\n",
        "for layer_name in activation_steerer.layer_names:\n",
        "    print(layer_name)\n",
        "\n",
        "    activation_steerer.clear_hooks()\n",
        "    added_tensor = torch.tensor(average_diff[layer_name]).to(activation_steerer.model.device)\n",
        "    if int(layer_name.split('.')[2]) == 15:\n",
        "        added_tensor = added_tensor * 1000\n",
        "    def add_to_last_index(x, added_tensor=added_tensor):\n",
        "        x[:, -1, :] += added_tensor\n",
        "        return x\n",
        "\n",
        "    activation_steerer.set_transformation_function(layer_name, add_to_last_index)\n",
        "    print(activation_steerer.generate(negative_essay_prompts[5], max_length=100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/60.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/243.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m243.4/243.4 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/420.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m420.1/420.1 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m60.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain_anthropic langchain_openai -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Macfarlane-Dick Evaluation ===\n",
            "{\n",
            "  \"overall_score\": 3.3,\n",
            "  \"principles\": [\n",
            "    {\n",
            "      \"principle\": \"Helps clarify what good performance is\",\n",
            "      \"score\": 3,\n",
            "      \"explanation\": \"The feedback provides some guidance on what constitutes good performance, such as including specific examples and citing sources, but it could be more comprehensive.\",\n",
            "      \"suggestion\": \"Include more specific criteria for what makes a strong essay, such as clear thesis statement, logical structure, and effective use of evidence.\"\n",
            "    },\n",
            "    {\n",
            "      \"principle\": \"Facilitates the development of self-assessment\",\n",
            "      \"score\": 2,\n",
            "      \"explanation\": \"The feedback doesn't explicitly encourage self-assessment, though it does point out areas for improvement which could prompt reflection.\",\n",
            "      \"suggestion\": \"Include questions that encourage the student to reflect on their work, such as 'How might you strengthen your arguments further?'\"\n",
            "    },\n",
            "    {\n",
            "      \"principle\": \"Delivers high-quality information to students about their learning\",\n",
            "      \"score\": 4,\n",
            "      \"explanation\": \"The feedback provides specific information about strengths and weaknesses in the essay, giving clear direction for improvement.\",\n",
            "      \"suggestion\": \"Provide more detailed examples of how the student could improve, perhaps by referencing specific parts of their essay.\"\n",
            "    },\n",
            "    {\n",
            "      \"principle\": \"Encourages teacher and peer dialogue around learning\",\n",
            "      \"score\": 1,\n",
            "      \"explanation\": \"The feedback doesn't explicitly encourage dialogue between the teacher and student or among peers.\",\n",
            "      \"suggestion\": \"Include an invitation for the student to discuss the feedback or ask questions, or suggest peer review activities.\"\n",
            "    },\n",
            "    {\n",
            "      \"principle\": \"Encourages positive motivational beliefs and self-esteem\",\n",
            "      \"score\": 4,\n",
            "      \"explanation\": \"The feedback begins with positive comments and uses constructive language throughout, which can boost motivation and self-esteem.\",\n",
            "      \"suggestion\": \"Balance critique with more specific praise to further boost motivation.\"\n",
            "    },\n",
            "    {\n",
            "      \"principle\": \"Provides opportunities to close the gap between current and desired performance\",\n",
            "      \"score\": 4,\n",
            "      \"explanation\": \"The feedback offers clear suggestions for improvement, giving the student direction on how to enhance their work.\",\n",
            "      \"suggestion\": \"Provide more specific strategies or resources the student could use to improve their essay.\"\n",
            "    },\n",
            "    {\n",
            "      \"principle\": \"Provides information to teachers that can be used to help shape teaching\",\n",
            "      \"score\": 2,\n",
            "      \"explanation\": \"While the feedback might indirectly inform teaching, it doesn't explicitly provide information for this purpose.\",\n",
            "      \"suggestion\": \"Include notes on common issues observed across multiple students' work to inform future teaching strategies.\"\n",
            "    }\n",
            "  ],\n",
            "  \"strengths\": [\n",
            "    \"Provides specific areas for improvement\",\n",
            "    \"Balances positive feedback with constructive criticism\",\n",
            "    \"Offers clear suggestions for enhancing the essay\"\n",
            "  ],\n",
            "  \"areas_for_improvement\": [\n",
            "    \"Encourage more self-assessment and reflection\",\n",
            "    \"Promote dialogue between teacher and student\",\n",
            "    \"Provide more detailed criteria for good performance\"\n",
            "  ],\n",
            "  \"summary\": \"This feedback demonstrates several strengths in providing clear, constructive guidance to the student. It effectively balances praise with areas for improvement and offers specific suggestions. However, it could be enhanced by encouraging more self-assessment, promoting dialogue, and providing more detailed criteria for excellence. Overall, it's a solid foundation for effective feedback that could be further improved to fully align with all seven principles of good feedback practice.\"\n",
            "}\n",
            "\n",
            "=== Feedback Comparison ===\n",
            "{\n",
            "  \"winner\": 1,\n",
            "  \"score_difference\": 7,\n",
            "  \"aspects\": [\n",
            "    {\n",
            "      \"aspect\": \"Clarity and specificity\",\n",
            "      \"feedback1_score\": 4,\n",
            "      \"feedback2_score\": 2,\n",
            "      \"explanation\": \"Feedback 1 provides clear, specific areas for improvement, while Feedback 2 is vague and general.\"\n",
            "    },\n",
            "    {\n",
            "      \"aspect\": \"Actionability\",\n",
            "      \"feedback1_score\": 5,\n",
            "      \"feedback2_score\": 1,\n",
            "      \"explanation\": \"Feedback 1 offers concrete suggestions for improvement, whereas Feedback 2 provides no actionable advice.\"\n",
            "    },\n",
            "    {\n",
            "      \"aspect\": \"Balance of positive and constructive elements\",\n",
            "      \"feedback1_score\": 4,\n",
            "      \"feedback2_score\": 0,\n",
            "      \"explanation\": \"Feedback 1 acknowledges strengths and areas for improvement, while Feedback 2 is entirely negative.\"\n",
            "    },\n",
            "    {\n",
            "      \"aspect\": \"Alignment with learning goals\",\n",
            "      \"feedback1_score\": 4,\n",
            "      \"feedback2_score\": 1,\n",
            "      \"explanation\": \"Feedback 1 addresses specific essay components, aligning with typical essay writing goals. Feedback 2 doesn't align with specific learning objectives.\"\n",
            "    },\n",
            "    {\n",
            "      \"aspect\": \"Potential impact on student motivation and learning\",\n",
            "      \"feedback1_score\": 4,\n",
            "      \"feedback2_score\": 1,\n",
            "      \"explanation\": \"Feedback 1 is encouraging and constructive, likely to motivate improvement. Feedback 2 is demotivating and may discourage the student.\"\n",
            "    }\n",
            "  ],\n",
            "  \"feedback1_strengths\": [\n",
            "    \"Provides specific areas for improvement\",\n",
            "    \"Offers actionable suggestions\",\n",
            "    \"Balances positive and constructive feedback\",\n",
            "    \"Aligns with essay writing goals\",\n",
            "    \"Encourages and motivates the student\"\n",
            "  ],\n",
            "  \"feedback1_weaknesses\": [\n",
            "    \"Could provide even more specific examples\"\n",
            "  ],\n",
            "  \"feedback2_strengths\": [\n",
            "    \"Communicates that significant improvement is needed\"\n",
            "  ],\n",
            "  \"feedback2_weaknesses\": [\n",
            "    \"Lacks specific, actionable advice\",\n",
            "    \"Entirely negative in tone\",\n",
            "    \"Does not align with specific learning goals\",\n",
            "    \"Potentially demotivating for the student\",\n",
            "    \"Fails to acknowledge any positive aspects of the work\"\n",
            "  ],\n",
            "  \"recommendation\": \"Feedback 2 could be improved by providing specific examples of weak arguments, identifying particular grammar errors, and offering concrete suggestions for improvement. It should also acknowledge any positive aspects of the essay, even if minor, to balance the critique and maintain student motivation. The feedback should align more closely with specific learning goals for essay writing and provide clear, actionable steps for revision.\"\n",
            "}\n",
            "\n",
            "=== Kindness Evaluation ===\n",
            "{\n",
            "  \"kindness_score\": 3.5,\n",
            "  \"explanation\": \"The feedback is generally kind and supportive, offering a balanced mix of positive recognition and constructive criticism. It acknowledges strengths while providing specific areas for improvement in a non-threatening manner.\",\n",
            "  \"aspects\": [\n",
            "    {\n",
            "      \"aspect\": \"Tone and language choice\",\n",
            "      \"score\": 4,\n",
            "      \"explanation\": \"The language is professional yet approachable, using phrases like 'good understanding' and 'I liked' to create a positive tone.\"\n",
            "    },\n",
            "    {\n",
            "      \"aspect\": \"Balance of positive and constructive elements\",\n",
            "      \"score\": 4,\n",
            "      \"explanation\": \"The feedback maintains a good balance, starting with positive observations before suggesting improvements, and ending with another positive note.\"\n",
            "    },\n",
            "    {\n",
            "      \"aspect\": \"Recognition of effort and achievement\",\n",
            "      \"score\": 3,\n",
            "      \"explanation\": \"The feedback acknowledges the student's good understanding and clear introduction, but could offer more specific praise for effort.\"\n",
            "    },\n",
            "    {\n",
            "      \"aspect\": \"Personalization\",\n",
            "      \"score\": 2,\n",
            "      \"explanation\": \"While the feedback addresses specific aspects of the essay, it lacks personal touches that would make it feel more individualized to the student.\"\n",
            "    },\n",
            "    {\n",
            "      \"aspect\": \"Empathy and understanding\",\n",
            "      \"score\": 3,\n",
            "      \"explanation\": \"The feedback shows some understanding of the student's work, but could demonstrate more empathy towards the challenges of essay writing.\"\n",
            "    }\n",
            "  ],\n",
            "  \"examples\": [\n",
            "    \"Your essay shows good understanding of the topic\",\n",
            "    \"I liked your introduction, it was clear and engaging\"\n",
            "  ],\n",
            "  \"improvement_suggestions\": [\n",
            "    \"Add more personalized comments, such as referring to specific content in the essay\",\n",
            "    \"Include encouragement for the effort put into the work\",\n",
            "    \"Offer more specific guidance on how to find relevant sources or examples\",\n",
            "    \"Use more empathetic language to acknowledge the challenges of academic writing\"\n",
            "  ],\n",
            "  \"summary\": \"The feedback is moderately kind, offering a good balance of positive recognition and constructive criticism. It maintains a supportive tone but could be improved with more personalization and empathy.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Dict, List, Any\n",
        "from langchain.llms import BaseLLM\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "\n",
        "class FeedbackEvaluator:\n",
        "    \"\"\"\n",
        "    A model-agnostic class to evaluate feedback using LangChain with various LLM providers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, provider: str = \"anthropic\", model_name: str = None):\n",
        "        \"\"\"\n",
        "        Initialize the FeedbackEvaluator with a specific LLM provider.\n",
        "\n",
        "        Args:\n",
        "            provider: The LLM provider to use ('anthropic', 'openai', etc.)\n",
        "            model_name: The specific model to use (optional, will use default if not specified)\n",
        "        \"\"\"\n",
        "        self.provider = provider\n",
        "        self.model_name = model_name\n",
        "        self.llm = self._initialize_llm()\n",
        "\n",
        "    def _initialize_llm(self) -> BaseLLM:\n",
        "        \"\"\"\n",
        "        Initialize the LLM based on the specified provider.\n",
        "\n",
        "        Returns:\n",
        "            A LangChain LLM instance\n",
        "        \"\"\"\n",
        "        if self.provider == \"anthropic\":\n",
        "            model = self.model_name or \"claude-3-5-sonnet-20240620\"\n",
        "            return ChatAnthropic(\n",
        "                anthropic_api_key=ANTHROPIC_KEY,\n",
        "                model=model,\n",
        "                temperature=0.2,\n",
        "                max_tokens=4000\n",
        "            )\n",
        "        elif self.provider == \"openai\":\n",
        "            model = self.model_name or \"gpt-4\"\n",
        "            return ChatOpenAI(\n",
        "                openai_api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        "                model=model,\n",
        "                temperature=0.2,\n",
        "                max_tokens=4000\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported provider: {self.provider}\")\n",
        "\n",
        "    def _call_llm(self, prompt: str, system_prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Call the LLM with a prompt and return the response.\n",
        "\n",
        "        Args:\n",
        "            prompt: The prompt to send to the LLM\n",
        "            system_prompt: The system prompt to use\n",
        "\n",
        "        Returns:\n",
        "            The LLM's response\n",
        "        \"\"\"\n",
        "        try:\n",
        "            messages = [\n",
        "                SystemMessage(content=system_prompt),\n",
        "                HumanMessage(content=prompt)\n",
        "            ]\n",
        "            response = self.llm.invoke(messages)\n",
        "            return response.content\n",
        "        except Exception as e:\n",
        "            print(f\"Error calling LLM: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def evaluate_macfarlane_dick(self, feedback: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate feedback based on Nicol & Macfarlane-Dick's seven principles of good feedback.\n",
        "\n",
        "        Args:\n",
        "            feedback: The feedback text to evaluate\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the evaluation results\n",
        "        \"\"\"\n",
        "        system_prompt = \"\"\"You are an expert in educational assessment and feedback evaluation.\n",
        "                Your task is to analyze feedback objectively and provide structured evaluations.\n",
        "                Provide your analysis in JSON format as specified in the prompt.\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Evaluate the following feedback according to Nicol & Macfarlane-Dick's seven principles of good feedback practice:\n",
        "\n",
        "        1. Helps clarify what good performance is\n",
        "        2. Facilitates the development of self-assessment\n",
        "        3. Delivers high-quality information to students about their learning\n",
        "        4. Encourages teacher and peer dialogue around learning\n",
        "        5. Encourages positive motivational beliefs and self-esteem\n",
        "        6. Provides opportunities to close the gap between current and desired performance\n",
        "        7. Provides information to teachers that can be used to help shape teaching\n",
        "\n",
        "        Feedback to evaluate:\n",
        "        \"{feedback}\"\n",
        "\n",
        "        For each principle, provide:\n",
        "        1. A score from 0 to 5 (where 0 is not at all and 5 is exemplary)\n",
        "        2. A brief explanation of why you gave this score\n",
        "        3. A suggestion for improvement\n",
        "\n",
        "        Return your evaluation as a JSON object with the following structure:\n",
        "        {{\n",
        "            \"overall_score\": <average of all scores, rounded to one decimal place>,\n",
        "            \"principles\": [\n",
        "                {{\n",
        "                    \"principle\": \"Principle 1\",\n",
        "                    \"score\": <score>,\n",
        "                    \"explanation\": \"<explanation>\",\n",
        "                    \"suggestion\": \"<suggestion>\"\n",
        "                }},\n",
        "                ...\n",
        "            ],\n",
        "            \"strengths\": [\"<strength 1>\", \"<strength 2>\", ...],\n",
        "            \"areas_for_improvement\": [\"<area 1>\", \"<area 2>\", ...],\n",
        "            \"summary\": \"<brief summary of evaluation>\"\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        response = self._call_llm(prompt, system_prompt)\n",
        "        try:\n",
        "            return json.loads(response)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Failed to parse JSON response from LLM\")\n",
        "            return {\n",
        "                \"error\": \"Failed to parse evaluation\",\n",
        "                \"raw_response\": response\n",
        "            }\n",
        "\n",
        "    def compare_feedback(self, feedback1: str, feedback2: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Compare two pieces of feedback and determine which is better.\n",
        "\n",
        "        Args:\n",
        "            feedback1: The first feedback text\n",
        "            feedback2: The second feedback text\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the comparison results\n",
        "        \"\"\"\n",
        "        system_prompt = \"\"\"You are an expert in educational assessment and feedback evaluation.\n",
        "                Your task is to analyze and compare feedback objectively.\n",
        "                Provide your analysis in JSON format as specified in the prompt.\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Compare the following two pieces of feedback and determine which one is more effective:\n",
        "\n",
        "        Feedback 1:\n",
        "        \"{feedback1}\"\n",
        "\n",
        "        Feedback 2:\n",
        "        \"{feedback2}\"\n",
        "\n",
        "        Consider the following aspects in your comparison:\n",
        "        1. Clarity and specificity\n",
        "        2. Actionability\n",
        "        3. Balance of positive and constructive elements\n",
        "        4. Alignment with learning goals\n",
        "        5. Potential impact on student motivation and learning\n",
        "\n",
        "        Return your comparison as a JSON object with the following structure:\n",
        "        {{\n",
        "            \"winner\": <1 or 2, indicating which feedback is better>,\n",
        "            \"score_difference\": <a number from 0 to 10 indicating how much better the winner is>,\n",
        "            \"aspects\": [\n",
        "                {{\n",
        "                    \"aspect\": \"Clarity and specificity\",\n",
        "                    \"feedback1_score\": <score from 0 to 5>,\n",
        "                    \"feedback2_score\": <score from 0 to 5>,\n",
        "                    \"explanation\": \"<explanation>\"\n",
        "                }},\n",
        "                ...\n",
        "            ],\n",
        "            \"feedback1_strengths\": [\"<strength 1>\", ...],\n",
        "            \"feedback1_weaknesses\": [\"<weakness 1>\", ...],\n",
        "            \"feedback2_strengths\": [\"<strength 1>\", ...],\n",
        "            \"feedback2_weaknesses\": [\"<weakness 1>\", ...],\n",
        "            \"recommendation\": \"<recommendation for how the lower-rated feedback could be improved>\"\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        response = self._call_llm(prompt, system_prompt)\n",
        "        try:\n",
        "            return json.loads(response)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Failed to parse JSON response from LLM\")\n",
        "            return {\n",
        "                \"error\": \"Failed to parse comparison\",\n",
        "                \"raw_response\": response\n",
        "            }\n",
        "\n",
        "    def evaluate_kindness(self, feedback: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Evaluate the kindness of feedback on a scale from 0 to 5.\n",
        "\n",
        "        Args:\n",
        "            feedback: The feedback text to evaluate\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the kindness evaluation\n",
        "        \"\"\"\n",
        "        system_prompt = \"\"\"You are an expert in educational assessment and feedback evaluation.\n",
        "                Your task is to analyze the kindness and empathy in feedback.\n",
        "                Provide your analysis in JSON format as specified in the prompt.\"\"\"\n",
        "\n",
        "        prompt = f\"\"\"\n",
        "        Evaluate the kindness of the following feedback on a scale from 0 to 5, where:\n",
        "\n",
        "        0: Harsh, potentially harmful or demotivating\n",
        "        1: Cold and impersonal\n",
        "        2: Neutral\n",
        "        3: Somewhat kind and supportive\n",
        "        4: Kind and encouraging\n",
        "        5: Exceptionally kind, empathetic and motivating\n",
        "\n",
        "        Feedback to evaluate:\n",
        "        \"{feedback}\"\n",
        "\n",
        "        Consider the following aspects of kindness in feedback:\n",
        "        1. Tone and language choice\n",
        "        2. Balance of positive and constructive elements\n",
        "        3. Recognition of effort and achievement\n",
        "        4. Personalization\n",
        "        5. Empathy and understanding\n",
        "\n",
        "        Return your evaluation as a JSON object with the following structure:\n",
        "        {{\n",
        "            \"kindness_score\": <score from 0 to 5>,\n",
        "            \"explanation\": \"<explanation for the score>\",\n",
        "            \"aspects\": [\n",
        "                {{\n",
        "                    \"aspect\": \"Tone and language choice\",\n",
        "                    \"score\": <score from 0 to 5>,\n",
        "                    \"explanation\": \"<explanation>\"\n",
        "                }},\n",
        "                ...\n",
        "            ],\n",
        "            \"examples\": [\"<example of kind/unkind language in the feedback>\", ...],\n",
        "            \"improvement_suggestions\": [\"<suggestion 1>\", ...],\n",
        "            \"summary\": \"<brief summary of kindness evaluation>\"\n",
        "        }}\n",
        "        \"\"\"\n",
        "\n",
        "        response = self._call_llm(prompt, system_prompt)\n",
        "        try:\n",
        "            return json.loads(response)\n",
        "        except json.JSONDecodeError:\n",
        "            print(\"Failed to parse JSON response from LLM\")\n",
        "            return {\n",
        "                \"error\": \"Failed to parse kindness evaluation\",\n",
        "                \"raw_response\": response\n",
        "            }\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Example feedback\n",
        "    sample_feedback = \"\"\"\n",
        "    Your essay shows good understanding of the topic, but your arguments need more support.\n",
        "    Try to include more specific examples and cite relevant sources to strengthen your points.\n",
        "    I liked your introduction, it was clear and engaging. Your conclusion could be stronger\n",
        "    by restating your main points and providing a thoughtful closing statement.\n",
        "    \"\"\"\n",
        "\n",
        "    # Create an evaluator\n",
        "    evaluator = FeedbackEvaluator(provider=\"anthropic\")\n",
        "\n",
        "    # Example 1: Evaluate using Macfarlane-Dick principles\n",
        "    print(\"\\n=== Macfarlane-Dick Evaluation ===\")\n",
        "    result = evaluator.evaluate_macfarlane_dick(sample_feedback)\n",
        "    print(json.dumps(result, indent=2))\n",
        "\n",
        "    # Example 2: Compare two pieces of feedback\n",
        "    sample_feedback2 = \"\"\"\n",
        "    This essay needs a lot of work. The arguments are weak and poorly supported.\n",
        "    Your writing style is confusing, and there are many grammar errors throughout.\n",
        "    You should completely revise this and try again.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"\\n=== Feedback Comparison ===\")\n",
        "    result = evaluator.compare_feedback(sample_feedback, sample_feedback2)\n",
        "    print(json.dumps(result, indent=2))\n",
        "\n",
        "    # Example 3: Evaluate kindness\n",
        "    print(\"\\n=== Kindness Evaluation ===\")\n",
        "    result = evaluator.evaluate_kindness(sample_feedback)\n",
        "    print(json.dumps(result, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FGSM Technique"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class LinearProbeSteerer(ActivationController):\n",
        "    def __init__(self, model, selected_blocks=None):\n",
        "        super().__init__(model, selected_blocks=selected_blocks)\n",
        "\n",
        "        self.best_layer = None\n",
        "        self.best_model = None\n",
        "        self.classifier = None\n",
        "\n",
        "    def train_classifier(self, positive_texts, negative_texts, device='cuda'):\n",
        "        positive_activations = self.extract_activations(positive_texts, device)\n",
        "        negative_activations = self.extract_activations(negative_texts, device)\n",
        "\n",
        "        results = {}\n",
        "        best_accuracy = 0\n",
        "        best_layer = \"\"\n",
        "        best_model = None\n",
        "        for layer_name in positive_activations:\n",
        "            positive_examples = np.array(positive_activations[layer_name])\n",
        "            negative_examples = np.array(negative_activations[layer_name])\n",
        "            print(\"Positive Examples Lne\", len(positive_examples))\n",
        "            print(\"Negative Examples Lne\", len(negative_examples))\n",
        "\n",
        "            labels = [1] * len(positive_examples) + [0] * len(positive_examples)\n",
        "\n",
        "            all_activations = list(positive_examples) + list(negative_examples)\n",
        "            all_activations = np.concatenate(all_activations, axis=0)\n",
        "\n",
        "            X_train, X_test, y_train, y_test = train_test_split(\n",
        "                all_activations, labels, test_size=0.3, random_state=42\n",
        "            )\n",
        "\n",
        "            # Train classifier\n",
        "            self.classifier.fit(X_train, y_train)\n",
        "            print(\"Y Train\", y_train[:10])\n",
        "\n",
        "            y_pred = self.classifier.predict(X_test)\n",
        "            print(\"y_pred\", y_pred[:5])\n",
        "            print(\"X_test\", X_test[:5])\n",
        "            report = classification_report(y_test, y_pred, output_dict=True)\n",
        "            results[layer_name] = report['accuracy']\n",
        "\n",
        "            print(f\"Accuracy: {report['accuracy']:.4f}\")\n",
        "            print(classification_report(y_test, y_pred))\n",
        "\n",
        "            if report['accuracy'] > best_accuracy:\n",
        "                best_accuracy = report['accuracy']\n",
        "                best_layer = layer_name\n",
        "                best_model = self.classifier\n",
        "\n",
        "        # Visualize results\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        layers = list(results.keys())\n",
        "        accuracies = [results[layer] for layer in layers]\n",
        "\n",
        "        plt.bar(range(len(results)), accuracies)\n",
        "        plt.xlabel('Layer')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.title('Classification Accuracy by Layer')\n",
        "        plt.xticks(range(len(results)), [f\"Layer {layer.split('.')[-2]} {layer.split('.')[-1]}\" for layer in layers])\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('steering_output/layer_accuracies.png')\n",
        "        print(\"Results visualization saved to 'steering_output/layer_accuracies.png'\")\n",
        "\n",
        "        print(\"Best model\", best_model.coef_)\n",
        "        self.best_model = best_model\n",
        "        self.best_layer = best_layer\n",
        "        self.classifier = best_model\n",
        "        return best_layer, best_model\n",
        "\n",
        "    def set_transformation_function(self, c=1, func_type='multiply'):\n",
        "        vector_gpu = torch.tensor(self.classifier.coef_[0], dtype=torch.float32)\n",
        "        vector_gpu = vector_gpu.to(activation_steerer.device)\n",
        "\n",
        "        def mult_func(activation_tensor, vec=vector_gpu, c=c):\n",
        "            return activation_tensor * (1 + (vec.to(activation_tensor.dtype ) * c))\n",
        "\n",
        "        def add_func(activation_tensor, vec=vector_gpu, c=c):\n",
        "            return activation_tensor + (vec.to(activation_tensor.dtype ) * c)\n",
        "\n",
        "        if func_type == \"add\":\n",
        "            return super().set_transformation_function(self.layer_name, add_func)\n",
        "        elif func_type == \"multiply\":\n",
        "            return super().set_transformation_function(self.layer_name, mult_func)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported function type: {func_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "class SimpleMLPProbe(nn.Module):\n",
        "    \"\"\"A simple linear probe for binary classification.\"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim=256, output_dim=256, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        # Linear layer mapping activation dimension to a single logit\n",
        "        self.linear = nn.Sequential(\n",
        "                nn.Linear(input_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, output_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(output_dim, 1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Output raw logits (suitable for BCEWithLogitsLoss)\n",
        "        return self.linear(x)\n",
        "    \n",
        "\n",
        "class TorchModelSteerer(ActivationController):\n",
        "    def __init__(self, model, selected_blocks=None,\n",
        "                 # Probe & Training Hyperparameters\n",
        "                 learning_rate=0.001, epochs=10, batch_size=32,\n",
        "                 probe_type='mlp',\n",
        "                 mlp_hidden_dim=None, mlp_dropout_prob=0.1):\n",
        "        super().__init__(model, selected_blocks=selected_blocks)\n",
        "\n",
        "        self.best_layer = None\n",
        "        self.best_model_state_dict = None\n",
        "        self.best_model_input_dim = None\n",
        "        self.best_probe_type = None # Store the type of the best probe\n",
        "\n",
        "        # Store hyperparameters\n",
        "        self.lr = learning_rate\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.probe_type = probe_type\n",
        "        self.mlp_hidden_dim = mlp_hidden_dim\n",
        "        self.mlp_dropout_prob = mlp_dropout_prob\n",
        "\n",
        "    def train_classifier(self, positive_texts, negative_texts, device='cuda'):\n",
        "        # Ensure the target device is a torch device\n",
        "        torch_device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device: {torch_device}\")\n",
        "\n",
        "        # Extract activations (assuming this returns a dict layer_name -> list of numpy arrays)\n",
        "        # Make sure activations are extracted correctly (e.g., mean pooled per text)\n",
        "        positive_activations = self.extract_activations(positive_texts, device) # Keep extraction device potentially different\n",
        "        negative_activations = self.extract_activations(negative_texts, device)\n",
        "\n",
        "        results = {}\n",
        "        best_accuracy = 0\n",
        "        best_layer = \"\"\n",
        "        best_model_state_dict = None\n",
        "        best_model_input_dim = None\n",
        "\n",
        "        for layer_name in positive_activations:\n",
        "            # Ensure we have activations for this layer from both sets\n",
        "            if layer_name not in negative_activations or \\\n",
        "               len(positive_activations[layer_name]) == 0 or \\\n",
        "               len(negative_activations[layer_name]) == 0:\n",
        "                print(f\"Skipping layer {layer_name}: Missing activations.\")\n",
        "                continue\n",
        "\n",
        "            # It's crucial that each item in the list is ONE vector per text input\n",
        "            # If extract_activations returns activations per token, you need to aggregate first (e.g., mean)\n",
        "            try:\n",
        "                # Assuming list contains numpy arrays ready to be stacked\n",
        "                positive_examples = np.vstack(positive_activations[layer_name])\n",
        "                negative_examples = np.vstack(negative_activations[layer_name])\n",
        "            except ValueError as e:\n",
        "                print(f\"Error stacking activations for layer {layer_name}: {e}\")\n",
        "                print(f\"Shapes: Pos {[a.shape for a in positive_activations[layer_name]]}, Neg {[a.shape for a in negative_activations[layer_name]]}\")\n",
        "                continue # Skip layer if shapes mismatch\n",
        "\n",
        "            print(f\"\\n--- Training Layer: {layer_name} ---\")\n",
        "            print(f\"Positive Examples shape: {positive_examples.shape}\")\n",
        "            print(f\"Negative Examples shape: {negative_examples.shape}\")\n",
        "\n",
        "\n",
        "            activation_dim = positive_examples.shape[1]\n",
        "\n",
        "            # --- Data Preparation ---\n",
        "            # Correct label creation\n",
        "            labels_pos = np.ones(len(positive_examples), dtype=np.float32)\n",
        "            labels_neg = np.zeros(len(negative_examples), dtype=np.float32)\n",
        "\n",
        "            all_activations_np = np.vstack((positive_examples, negative_examples))\n",
        "            all_labels_np = np.concatenate((labels_pos, labels_neg))\n",
        "\n",
        "            # Split data\n",
        "            X_train_np, X_test_np, y_train_np, y_test_np = train_test_split(\n",
        "                all_activations_np, all_labels_np, test_size=0.3, random_state=42, stratify=all_labels_np\n",
        "            )\n",
        "\n",
        "            # Convert to PyTorch Tensors\n",
        "            X_train = torch.tensor(X_train_np, dtype=torch.float32)\n",
        "            y_train = torch.tensor(y_train_np, dtype=torch.float32).unsqueeze(1) # Add dim for BCEWithLogitsLoss\n",
        "            X_test = torch.tensor(X_test_np, dtype=torch.float32)\n",
        "            y_test = torch.tensor(y_test_np, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "            # Create DataLoaders\n",
        "            train_dataset = TensorDataset(X_train, y_train)\n",
        "            test_dataset = TensorDataset(X_test, y_test)\n",
        "            train_loader = DataLoader(train_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "            test_loader = DataLoader(test_dataset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "            # --- Model, Loss, Optimizer ---\n",
        "            probe_model = SimpleMLPProbe(activation_dim).to(torch_device)\n",
        "            criterion = nn.BCEWithLogitsLoss() # Numerically stable\n",
        "            optimizer = optim.Adam(probe_model.parameters(), lr=self.lr)\n",
        "\n",
        "            # --- Training Loop ---\n",
        "            probe_model.train()\n",
        "            for epoch in range(self.epochs):\n",
        "                epoch_loss = 0\n",
        "                for batch_X, batch_y in train_loader:\n",
        "                    batch_X, batch_y = batch_X.to(torch_device), batch_y.to(torch_device)\n",
        "\n",
        "                    optimizer.zero_grad()\n",
        "                    outputs = probe_model(batch_X)\n",
        "                    loss = criterion(outputs, batch_y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                    epoch_loss += loss.item()\n",
        "                # print(f\"Epoch [{epoch+1}/{self.epochs}], Loss: {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "            # --- Evaluation ---\n",
        "            probe_model.eval()\n",
        "            all_preds = []\n",
        "            all_targets = []\n",
        "            with torch.no_grad():\n",
        "                for batch_X, batch_y in test_loader:\n",
        "                    batch_X = batch_X.to(torch_device)\n",
        "                    outputs = probe_model(batch_X)\n",
        "                    # Convert logits to probabilities then to predictions (0 or 1)\n",
        "                    preds = (torch.sigmoid(outputs) > 0.5).cpu().long().squeeze().numpy()\n",
        "                    all_preds.extend(preds.tolist())\n",
        "                    all_targets.extend(batch_y.cpu().long().squeeze().numpy().tolist())\n",
        "\n",
        "            # Calculate accuracy and report\n",
        "            # Ensure consistent types if using lists directly\n",
        "            try:\n",
        "                accuracy = accuracy_score(all_targets, all_preds)\n",
        "                report_str = classification_report(all_targets, all_preds)\n",
        "                print(f\"Accuracy: {accuracy:.4f}\")\n",
        "                print(report_str)\n",
        "                results[layer_name] = accuracy\n",
        "\n",
        "                # --- Update Best Model ---\n",
        "                if accuracy > best_accuracy:\n",
        "                    best_accuracy = accuracy\n",
        "                    best_layer = layer_name\n",
        "                    # Save the state_dict and input dim\n",
        "                    best_model_state_dict = copy.deepcopy(probe_model.state_dict())\n",
        "                    best_model_input_dim = activation_dim\n",
        "                    print(f\"*** New best layer: {best_layer} (Accuracy: {best_accuracy:.4f}) ***\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error during evaluation for layer {layer_name}: {e}\")\n",
        "                results[layer_name] = 0.0 # Assign low score if evaluation fails\n",
        "\n",
        "\n",
        "        # --- Visualization (remains the same) ---\n",
        "        if results:\n",
        "          plt.figure(figsize=(10, 6))\n",
        "          layers = list(results.keys())\n",
        "          accuracies = [results[layer] for layer in layers]\n",
        "\n",
        "          plt.bar(range(len(results)), accuracies)\n",
        "          plt.xlabel('Layer')\n",
        "          plt.ylabel('Accuracy')\n",
        "          plt.title('Classification Accuracy by Layer (PyTorch Probe)')\n",
        "          # Adjust labels if needed, e.g., rotate\n",
        "          plt.xticks(range(len(results)), [f\"L {'.'.join(layer.split('.')[-2:])}\" for layer in layers], rotation=45, ha='right')\n",
        "          plt.tight_layout()\n",
        "          plt.savefig('steering_output/layer_accuracies_pytorch.png')\n",
        "          print(\"Results visualization saved to 'steering_output/layer_accuracies_pytorch.png'\")\n",
        "        else:\n",
        "          print(\"No results to plot.\")\n",
        "\n",
        "\n",
        "        # Store best model info in the instance\n",
        "        self.best_model_state_dict = best_model_state_dict\n",
        "        self.best_model_input_dim = best_model_input_dim\n",
        "        self.best_layer = best_layer\n",
        "\n",
        "        if self.best_layer:\n",
        "            print(f\"\\nBest overall layer: {self.best_layer} with accuracy: {best_accuracy:.4f}\")\n",
        "        else:\n",
        "            print(\"\\nFailed to find a suitable layer for steering.\")\n",
        "\n",
        "        # Return layer name and best accuracy (or model info if needed)\n",
        "        return self.best_layer, best_accuracy\n",
        "\n",
        "\n",
        "    \n",
        "    @torch.no_grad() # Disable gradients for the main generation loop\n",
        "    def generate(self, tokenizer, prompt,\n",
        "                 max_length=50, pnp_step_size=0.02, pnp_target_label=1.0,\n",
        "                 temperature=1.0, top_k=50, device=None):\n",
        "\n",
        "        \"\"\"Generates text using PnP steering with the trained probe.\"\"\"\n",
        "\n",
        "\n",
        "        if device is None:\n",
        "            device = self.model.device if hasattr(self.model, 'device') else \\\n",
        "                     torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "        probe_model = SimpleMLPProbe(\n",
        "            input_dim=self.best_model_input_dim,\n",
        "            hidden_dim=self.mlp_hidden_dim, # Relies on stored param\n",
        "            dropout_prob=self.mlp_dropout_prob # Relies on stored param\n",
        "        ).to(device)\n",
        "        probe_model.load_state_dict(self.best_model_state_dict)\n",
        "        probe_model.eval()\n",
        "\n",
        "        # 2. Prepare inputs\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=self.model.config.max_position_embeddings - max_length).to(device)\n",
        "        input_ids = inputs.input_ids\n",
        "        attention_mask = inputs.attention_mask\n",
        "        past_key_values = None\n",
        "        generated_ids = input_ids.clone()\n",
        "\n",
        "        # --- 4. Generation Loop ---\n",
        "        try:\n",
        "            for step in range(max_length):\n",
        "                \n",
        "                activations = self.extract_activations(prompt, device)\n",
        "\n",
        "\n",
        "                # --- Forward pass (captures state via hook) ---\n",
        "                # Enable gradients temporarily ONLY for PnP update if needed below\n",
        "                outputs = self.model(**model_inputs, output_hidden_states=False, use_cache=True)\n",
        "                next_token_logits = outputs.logits[:, -1, :] # Logits for the last token\n",
        "                past_key_values = outputs.past_key_values\n",
        "\n",
        "                # --- PnP Update Step ---\n",
        "                h_t = captured_state_dict['value'] # State from hook\n",
        "                if h_t is not None:\n",
        "                    # Enable gradient calculation for this part\n",
        "                    with torch.enable_grad():\n",
        "                        h_t = h_t.requires_grad_(True)\n",
        "\n",
        "                        # Forward through probe\n",
        "                        logits_probe = probe_model(h_t)\n",
        "                        criterion = nn.BCEWithLogitsLoss()\n",
        "                        # Ensure target label is float and on correct device\n",
        "                        target = torch.full_like(logits_probe, float(pnp_target_label), device=device)\n",
        "                        loss = criterion(logits_probe, target)\n",
        "\n",
        "                        # Backward pass to get gradient w.r.t. h_t\n",
        "                        loss.backward()\n",
        "                        grad = h_t.grad\n",
        "\n",
        "                    # --- Update hidden state ---\n",
        "                    if grad is not None:\n",
        "                        norm_grad = grad / (grad.norm(p=2, dim=-1, keepdim=True) + 1e-8)\n",
        "                        # Update happens outside grad context\n",
        "                        h_t_updated = h_t.detach() - pnp_step_size * norm_grad\n",
        "\n",
        "                        # --- Use updated state to modify logits (Model Dependent!) ---\n",
        "                        # This is the hardest part to generalize.\n",
        "                        # Option A: Rerun LM Head (if accessible & state shape matches)\n",
        "                        try:\n",
        "                            # Check if lm_head expects [batch, seq, hidden] or [batch, hidden]\n",
        "                            if hasattr(self.model, 'lm_head') and callable(self.model.lm_head):\n",
        "                                if h_t_updated.dim() == 2: # [batch, hidden] -> [batch, 1, hidden]\n",
        "                                   h_t_updated_seq = h_t_updated.unsqueeze(1)\n",
        "                                elif h_t_updated.dim() == 3: # Assume shape is already ok\n",
        "                                   h_t_updated_seq = h_t_updated\n",
        "                                else: raise ValueError(\"Unexpected state dimension\")\n",
        "\n",
        "                                # Recalculate logits for the last position\n",
        "                                next_token_logits = self.model.lm_head(h_t_updated_seq).squeeze(1) # [batch, vocab]\n",
        "                                print(f\"Step {step}: Recalculated logits.\") # Debug print\n",
        "                            else:\n",
        "                                print(f\"Step {step}: Cannot access lm_head directly, skipping logit recalculation.\")\n",
        "\n",
        "                        except Exception as e:\n",
        "                             print(f\"Step {step}: Error recalculating logits with updated state: {e}. Using original logits.\")\n",
        "\n",
        "                        # Option B: Perturb original logits (Approximation)\n",
        "                        # Requires calculating grad w.r.t logits or using W_lm * grad approx.\n",
        "                        # (Not implemented here for simplicity)\n",
        "\n",
        "                        # --- Clean Gradients (on original h_t) ---\n",
        "                        # Not strictly needed as h_t is recreated each loop, but good practice\n",
        "                        # if h_t had persistent graph connections.\n",
        "\n",
        "                    # Reset captured state for next iteration\n",
        "                    captured_state_dict['value'] = None\n",
        "                else:\n",
        "                     print(f\"Warning: Step {step}: Did not capture hidden state.\")\n",
        "\n",
        "                # --- Token Sampling (outside grad context) ---\n",
        "                if temperature != 1.0:\n",
        "                    next_token_logits = next_token_logits / temperature\n",
        "\n",
        "                if top_k > 0:\n",
        "                    # Slightly safer top-k implementation\n",
        "                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
        "                    next_token_logits[indices_to_remove] = -float('Inf')\n",
        "\n",
        "                probs = F.softmax(next_token_logits, dim=-1)\n",
        "                next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                # --- Update generated sequence ---\n",
        "                generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
        "                if attention_mask is not None:\n",
        "                    # Pad attention mask if needed by model.prepare_inputs\n",
        "                    attention_mask = torch.cat([attention_mask, torch.ones_like(next_token_id)], dim=1)\n",
        "\n",
        "                # --- Check for stopping ---\n",
        "                if tokenizer.eos_token_id is not None and (next_token_id == tokenizer.eos_token_id).all():\n",
        "                    print(\"EOS token generated.\")\n",
        "                    break\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation loop: {e}\")\n",
        "            # Ensure hook removal even if loop fails\n",
        "\n",
        "        # 6. Decode result\n",
        "        return tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "and positive.\n",
            "\n",
            "Wait, the original essay is a bit jumbled. Maybe I should try to fix the structure.\n",
            "\n",
            "Okay,\n",
            "and maybe fix the problem.\n",
            "\n",
            "Wait, the user is talking about the fact that school projects are often teacher-designed, which the\n",
            "and doesn't make the writer sound like they don't like the reader.\n",
            "\n",
            "Okay, so I need to write a response to\n",
            "Good\n",
            ".\n",
            "\n",
            "Alright, so if everything is about to be all about the students. If they're going to give students projects over the\n",
            ".\n",
            "\n",
            "So, if the student is required to do projects over the summer break. They feel like it's a positive effect on\n",
            ".\n",
            "\n",
            "So, wrapping this up, I think the answer is that projects should be student designed because students get to show their full\n",
            "Bad\n",
            ".\n",
            "\n",
            "Wait, but let me think. If a student is given a project that they don't like, they might not like\n",
            ". But in any case, the project must be student designed, so that the students can do as they please, as long\n",
            ".\n",
            "\n",
            "But why would a student not like doing this project? Because the teachers don't like it. No, that's not\n"
          ]
        }
      ],
      "source": [
        "vector_gpu = torch.tensor(activation_steerer.classifier.coef_[0], dtype=torch.float32)\n",
        "vector_gpu = vector_gpu.to(activation_steerer.device)\n",
        "\n",
        "\n",
        "activation_steerer.clear_hooks()\n",
        "prompt = encourageing_essay_prompts[0]\n",
        "max_length = 25\n",
        "print(activation_steerer.generate(prompt, max_length=max_length))\n",
        "print(activation_steerer.generate(prompt, max_length=max_length))\n",
        "print(activation_steerer.generate(prompt, max_length=max_length))\n",
        "\n",
        "\n",
        "def transformation_func(activation_tensor, vec=vector_gpu, c=150):\n",
        "    return activation_tensor * (1 + (vec.to(activation_tensor.dtype ) * c))\n",
        "\n",
        "activation_steerer.set_transformation_function(best_layer, transformation_func)\n",
        "\n",
        "prompt = encourageing_essay_prompts[0]\n",
        "max_length = 25\n",
        "print(\"Good\")\n",
        "print(activation_steerer.generate(prompt, max_length=max_length))\n",
        "print(activation_steerer.generate(prompt, max_length=max_length))\n",
        "print(activation_steerer.generate(prompt, max_length=max_length))\n",
        "\n",
        "\n",
        "def transformation_func(activation_tensor, vec=vector_gpu, c=-150):\n",
        "    # Add the steering vector (broadcasting should handle dimensions)\n",
        "    # Ensure dtype compatibility if using AMP\n",
        "\n",
        "    return activation_tensor * (1 + (vec.to(activation_tensor.dtype ) * c))\n",
        "activation_steerer.set_transformation_function(best_layer, transformation_func)\n",
        "\n",
        "prompt = encourageing_essay_prompts[0]\n",
        "max_length = 25\n",
        "print(\"Bad\")\n",
        "print(activation_steerer.generate(prompt, max_length=max_length))\n",
        "print(activation_steerer.generate(prompt, max_length=max_length))\n",
        "print(activation_steerer.generate(prompt, max_length=max_length))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from sparsify import SaeConfig, Trainer, TrainConfig\n",
        "from sparsify.data import chunk_and_tokenize\n",
        "\n",
        "MODEL = \"HuggingFaceTB/SmolLM2-135M\"\n",
        "dataset = load_dataset(\n",
        "    \"EleutherAI/fineweb-edu-dedup-10b\", split=\"train\",\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "tokenized = chunk_and_tokenize(dataset, tokenizer)\n",
        "\n",
        "\n",
        "gpt = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    device_map={\"\": \"cuda\"},\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "cfg = TrainConfig(SaeConfig(), batch_size=16)\n",
        "trainer = Trainer(cfg, tokenized, gpt)\n",
        "\n",
        "trainer.fit()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
